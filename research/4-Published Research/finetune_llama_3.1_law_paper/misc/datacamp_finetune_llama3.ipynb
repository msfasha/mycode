{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.datacamp.com/tutorial/llama3-fine-tuning-locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (4.42.4)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.44.2-py3-none-any.whl.metadata (43 kB)\n",
      "     ---------------------------------------- 0.0/43.7 kB ? eta -:--:--\n",
      "     ----------------- -------------------- 20.5/43.7 kB 330.3 kB/s eta 0:00:01\n",
      "     -------------------------------------- 43.7/43.7 kB 428.4 kB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (0.23.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (2.32.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests->transformers) (2024.7.4)\n",
      "Downloading transformers-4.44.2-py3-none-any.whl (9.5 MB)\n",
      "   ---------------------------------------- 0.0/9.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/9.5 MB 2.0 MB/s eta 0:00:05\n",
      "    --------------------------------------- 0.2/9.5 MB 2.9 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 0.5/9.5 MB 3.6 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 1.0/9.5 MB 5.5 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 1.5/9.5 MB 7.4 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 2.2/9.5 MB 9.2 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 3.0/9.5 MB 9.4 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 3.6/9.5 MB 11.3 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 3.6/9.5 MB 11.3 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 6.3/9.5 MB 13.8 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 7.0/9.5 MB 13.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 8.1/9.5 MB 14.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 9.2/9.5 MB 15.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.5/9.5 MB 14.4 MB/s eta 0:00:00\n",
      "Installing collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.42.4\n",
      "    Uninstalling transformers-4.42.4:\n",
      "      Successfully uninstalled transformers-4.42.4\n",
      "Successfully installed transformers-4.44.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: datasets in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (2.20.0)\n",
      "Collecting datasets\n",
      "  Downloading datasets-2.21.0-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from datasets) (1.24.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from datasets) (2.32.2)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: xxhash in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.3.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from datasets) (0.23.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from aiohttp->datasets) (1.9.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests>=2.32.2->datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests>=2.32.2->datasets) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading datasets-2.21.0-py3-none-any.whl (527 kB)\n",
      "   ---------------------------------------- 0.0/527.3 kB ? eta -:--:--\n",
      "   --- ----------------------------------- 41.0/527.3 kB 991.0 kB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 153.6/527.3 kB 1.8 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 317.4/527.3 kB 2.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 527.3/527.3 kB 3.0 MB/s eta 0:00:00\n",
      "Installing collected packages: datasets\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 2.20.0\n",
      "    Uninstalling datasets-2.20.0:\n",
      "      Successfully uninstalled datasets-2.20.0\n",
      "Successfully installed datasets-2.21.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: accelerate in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (0.32.1)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-0.34.2-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from accelerate) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from accelerate) (24.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from accelerate) (6.0.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from accelerate) (2.3.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from accelerate) (0.23.4)\n",
      "Collecting safetensors>=0.4.3 (from accelerate)\n",
      "  Downloading safetensors-0.4.5-cp311-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.3.1)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.2)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch>=1.10.0->accelerate) (2021.4.0)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.10.0->accelerate) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.10.0->accelerate) (2021.13.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.21.0->accelerate) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.7.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Downloading accelerate-0.34.2-py3-none-any.whl (324 kB)\n",
      "   ---------------------------------------- 0.0/324.4 kB ? eta -:--:--\n",
      "   --- ------------------------------------ 30.7/324.4 kB 1.3 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 112.6/324.4 kB 1.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 276.5/324.4 kB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 324.4/324.4 kB 2.2 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.4.5-cp311-none-win_amd64.whl (285 kB)\n",
      "   ---------------------------------------- 0.0/286.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 286.0/286.0 kB ? eta 0:00:00\n",
      "Installing collected packages: safetensors, accelerate\n",
      "  Attempting uninstall: safetensors\n",
      "    Found existing installation: safetensors 0.4.2\n",
      "    Uninstalling safetensors-0.4.2:\n",
      "      Successfully uninstalled safetensors-0.4.2\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 0.32.1\n",
      "    Uninstalling accelerate-0.32.1:\n",
      "      Successfully uninstalled accelerate-0.32.1\n",
      "Successfully installed accelerate-0.34.2 safetensors-0.4.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: peft in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (0.11.1)\n",
      "Collecting peft\n",
      "  Downloading peft-0.12.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from peft) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from peft) (24.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from peft) (6.0.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from peft) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from peft) (2.3.1)\n",
      "Requirement already satisfied: transformers in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from peft) (4.44.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from peft) (4.66.4)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from peft) (0.34.2)\n",
      "Requirement already satisfied: safetensors in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from peft) (0.4.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from peft) (0.23.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from huggingface-hub>=0.17.0->peft) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from huggingface-hub>=0.17.0->peft) (2024.3.1)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from huggingface-hub>=0.17.0->peft) (2.32.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from huggingface-hub>=0.17.0->peft) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch>=1.13.0->peft) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch>=1.13.0->peft) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch>=1.13.0->peft) (3.1.4)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch>=1.13.0->peft) (2021.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from tqdm->peft) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers->peft) (2023.10.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers->peft) (0.19.1)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.13.0->peft) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.13.0->peft) (2021.13.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests->huggingface-hub>=0.17.0->peft) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests->huggingface-hub>=0.17.0->peft) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.7.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
      "Downloading peft-0.12.0-py3-none-any.whl (296 kB)\n",
      "   ---------------------------------------- 0.0/296.4 kB ? eta -:--:--\n",
      "   ---- ---------------------------------- 30.7/296.4 kB 660.6 kB/s eta 0:00:01\n",
      "   ------------ --------------------------- 92.2/296.4 kB 1.1 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 225.3/296.4 kB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 296.4/296.4 kB 2.0 MB/s eta 0:00:00\n",
      "Installing collected packages: peft\n",
      "  Attempting uninstall: peft\n",
      "    Found existing installation: peft 0.11.1\n",
      "    Uninstalling peft-0.11.1:\n",
      "      Successfully uninstalled peft-0.11.1\n",
      "Successfully installed peft-0.12.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: trl in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (0.9.6)\n",
      "Collecting trl\n",
      "  Downloading trl-0.10.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: torch>=1.4.0 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from trl) (2.3.1)\n",
      "Requirement already satisfied: transformers>=4.31.0 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from trl) (4.44.2)\n",
      "Requirement already satisfied: numpy>=1.18.2 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from trl) (1.24.3)\n",
      "Requirement already satisfied: accelerate in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from trl) (0.34.2)\n",
      "Requirement already satisfied: datasets in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from trl) (2.21.0)\n",
      "Requirement already satisfied: tyro>=0.5.11 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from trl) (0.8.5)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch>=1.4.0->trl) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch>=1.4.0->trl) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch>=1.4.0->trl) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch>=1.4.0->trl) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch>=1.4.0->trl) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch>=1.4.0->trl) (2024.3.1)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch>=1.4.0->trl) (2021.4.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers>=4.31.0->trl) (0.23.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers>=4.31.0->trl) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers>=4.31.0->trl) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers>=4.31.0->trl) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers>=4.31.0->trl) (2.32.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers>=4.31.0->trl) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers>=4.31.0->trl) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers>=4.31.0->trl) (4.66.4)\n",
      "Requirement already satisfied: docstring-parser>=0.16 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from tyro>=0.5.11->trl) (0.16)\n",
      "Requirement already satisfied: rich>=11.1.0 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from tyro>=0.5.11->trl) (13.7.1)\n",
      "Requirement already satisfied: shtab>=1.5.6 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from tyro>=0.5.11->trl) (1.7.1)\n",
      "Requirement already satisfied: colorama>=0.4.0 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from tyro>=0.5.11->trl) (0.4.6)\n",
      "Requirement already satisfied: psutil in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from accelerate->trl) (6.0.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from datasets->trl) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from datasets->trl) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from datasets->trl) (2.2.2)\n",
      "Requirement already satisfied: xxhash in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from datasets->trl) (2.0.2)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from datasets->trl) (0.70.15)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from datasets->trl) (3.9.5)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from aiohttp->datasets->trl) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from aiohttp->datasets->trl) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from aiohttp->datasets->trl) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from aiohttp->datasets->trl) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from aiohttp->datasets->trl) (1.9.3)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.4.0->trl) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.4.0->trl) (2021.13.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests->transformers>=4.31.0->trl) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests->transformers>=4.31.0->trl) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests->transformers>=4.31.0->trl) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests->transformers>=4.31.0->trl) (2024.7.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.18.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from jinja2->torch>=1.4.0->trl) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from pandas->datasets->trl) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from pandas->datasets->trl) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from pandas->datasets->trl) (2023.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from sympy->torch>=1.4.0->trl) (1.3.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets->trl) (1.16.0)\n",
      "Downloading trl-0.10.1-py3-none-any.whl (280 kB)\n",
      "   ---------------------------------------- 0.0/280.1 kB ? eta -:--:--\n",
      "   -- ------------------------------------ 20.5/280.1 kB 640.0 kB/s eta 0:00:01\n",
      "   -------- ------------------------------ 61.4/280.1 kB 812.7 kB/s eta 0:00:01\n",
      "   ------------------------ --------------- 174.1/280.1 kB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 280.1/280.1 kB 1.7 MB/s eta 0:00:00\n",
      "Installing collected packages: trl\n",
      "  Attempting uninstall: trl\n",
      "    Found existing installation: trl 0.9.6\n",
      "    Uninstalling trl-0.9.6:\n",
      "      Successfully uninstalled trl-0.9.6\n",
      "Successfully installed trl-0.10.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: bitsandbytes in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (0.43.1)\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.43.3-py3-none-win_amd64.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: torch in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from bitsandbytes) (2.3.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from bitsandbytes) (1.24.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch->bitsandbytes) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch->bitsandbytes) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch->bitsandbytes) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch->bitsandbytes) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch->bitsandbytes) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch->bitsandbytes) (2024.3.1)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch->bitsandbytes) (2021.4.0)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch->bitsandbytes) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch->bitsandbytes) (2021.13.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from jinja2->torch->bitsandbytes) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from sympy->torch->bitsandbytes) (1.3.0)\n",
      "Downloading bitsandbytes-0.43.3-py3-none-win_amd64.whl (136.5 MB)\n",
      "   ---------------------------------------- 0.0/136.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/136.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/136.5 MB 656.4 kB/s eta 0:03:28\n",
      "   ---------------------------------------- 0.2/136.5 MB 1.1 MB/s eta 0:01:59\n",
      "   ---------------------------------------- 0.4/136.5 MB 2.0 MB/s eta 0:01:08\n",
      "   ---------------------------------------- 0.7/136.5 MB 3.2 MB/s eta 0:00:42\n",
      "   ---------------------------------------- 1.4/136.5 MB 5.3 MB/s eta 0:00:26\n",
      "    --------------------------------------- 2.4/136.5 MB 7.5 MB/s eta 0:00:18\n",
      "    --------------------------------------- 2.8/136.5 MB 8.5 MB/s eta 0:00:16\n",
      "    --------------------------------------- 2.8/136.5 MB 7.2 MB/s eta 0:00:19\n",
      "   - -------------------------------------- 4.0/136.5 MB 8.8 MB/s eta 0:00:16\n",
      "   - -------------------------------------- 4.5/136.5 MB 8.9 MB/s eta 0:00:15\n",
      "   - -------------------------------------- 4.9/136.5 MB 9.0 MB/s eta 0:00:15\n",
      "   - -------------------------------------- 5.3/136.5 MB 8.9 MB/s eta 0:00:15\n",
      "   - -------------------------------------- 5.7/136.5 MB 8.6 MB/s eta 0:00:16\n",
      "   - -------------------------------------- 5.9/136.5 MB 8.9 MB/s eta 0:00:15\n",
      "   -- ------------------------------------- 7.3/136.5 MB 9.9 MB/s eta 0:00:14\n",
      "   -- ------------------------------------- 8.6/136.5 MB 11.2 MB/s eta 0:00:12\n",
      "   -- ------------------------------------- 8.9/136.5 MB 10.8 MB/s eta 0:00:12\n",
      "   --- ------------------------------------ 10.9/136.5 MB 15.6 MB/s eta 0:00:09\n",
      "   --- ------------------------------------ 11.7/136.5 MB 16.0 MB/s eta 0:00:08\n",
      "   --- ------------------------------------ 12.7/136.5 MB 16.0 MB/s eta 0:00:08\n",
      "   --- ------------------------------------ 13.5/136.5 MB 18.2 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 14.4/136.5 MB 18.2 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 15.3/136.5 MB 19.8 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 16.2/136.5 MB 22.6 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 17.2/136.5 MB 22.6 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 18.2/136.5 MB 20.5 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 19.2/136.5 MB 21.8 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 20.2/136.5 MB 21.1 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 21.2/136.5 MB 19.9 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 22.1/136.5 MB 19.9 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 23.1/136.5 MB 20.5 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 24.1/136.5 MB 20.5 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 25.0/136.5 MB 20.5 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 26.0/136.5 MB 20.5 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 27.0/136.5 MB 19.9 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 28.0/136.5 MB 20.5 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 28.9/136.5 MB 21.1 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 29.9/136.5 MB 19.8 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 30.9/136.5 MB 19.8 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 31.9/136.5 MB 20.5 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 32.9/136.5 MB 20.5 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 33.9/136.5 MB 20.5 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 34.9/136.5 MB 21.1 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 36.0/136.5 MB 21.1 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 37.0/136.5 MB 21.1 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 38.1/136.5 MB 21.1 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 39.1/136.5 MB 21.1 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 40.1/136.5 MB 21.8 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 41.1/136.5 MB 21.1 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 42.1/136.5 MB 21.9 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 43.1/136.5 MB 21.8 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 44.1/136.5 MB 21.1 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 45.2/136.5 MB 21.1 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 46.2/136.5 MB 21.8 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 47.2/136.5 MB 21.1 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 48.3/136.5 MB 21.1 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 49.3/136.5 MB 21.1 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 50.3/136.5 MB 21.1 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 51.3/136.5 MB 21.8 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 52.4/136.5 MB 21.8 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 53.4/136.5 MB 21.8 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 54.4/136.5 MB 21.9 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 55.4/136.5 MB 22.6 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 56.2/136.5 MB 21.8 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 57.3/136.5 MB 21.9 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 58.4/136.5 MB 22.6 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 59.4/136.5 MB 22.6 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 60.5/136.5 MB 22.6 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 61.6/136.5 MB 21.8 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 62.6/136.5 MB 21.9 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 63.7/136.5 MB 21.9 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 64.7/136.5 MB 21.8 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 65.8/136.5 MB 22.6 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 67.0/136.5 MB 21.8 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 68.0/136.5 MB 22.6 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 69.1/136.5 MB 22.6 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 70.2/136.5 MB 22.5 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 71.2/136.5 MB 22.6 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 72.3/136.5 MB 22.6 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 73.3/136.5 MB 22.6 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 74.4/136.5 MB 22.6 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 75.4/136.5 MB 22.6 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 76.5/136.5 MB 22.6 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 77.5/136.5 MB 22.6 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 78.5/136.5 MB 22.5 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 79.6/136.5 MB 22.6 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 80.7/136.5 MB 22.6 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 81.7/136.5 MB 22.6 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 82.9/136.5 MB 22.6 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 83.9/136.5 MB 22.6 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 85.0/136.5 MB 22.6 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 86.1/136.5 MB 22.6 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 86.5/136.5 MB 22.6 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 87.6/136.5 MB 21.1 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 88.6/136.5 MB 21.1 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 89.5/136.5 MB 21.1 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 90.4/136.5 MB 21.1 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 91.4/136.5 MB 21.1 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 92.5/136.5 MB 21.1 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 93.6/136.5 MB 21.1 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 94.8/136.5 MB 21.8 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 95.7/136.5 MB 21.8 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 96.7/136.5 MB 21.8 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 97.8/136.5 MB 23.4 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 98.9/136.5 MB 23.4 MB/s eta 0:00:02\n",
      "   ---------------------------- ---------- 100.0/136.5 MB 22.6 MB/s eta 0:00:02\n",
      "   ---------------------------- ---------- 101.1/136.5 MB 22.6 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 102.2/136.5 MB 22.6 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 103.3/136.5 MB 22.6 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 104.4/136.5 MB 23.4 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 105.5/136.5 MB 23.4 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 106.6/136.5 MB 23.4 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 107.7/136.5 MB 24.2 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 108.9/136.5 MB 23.4 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 110.0/136.5 MB 23.4 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 111.0/136.5 MB 23.4 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 112.1/136.5 MB 23.4 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 113.3/136.5 MB 23.4 MB/s eta 0:00:01\n",
      "   -------------------------------- ------ 114.4/136.5 MB 23.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 115.5/136.5 MB 23.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 116.6/136.5 MB 23.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 117.7/136.5 MB 23.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 118.9/136.5 MB 23.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 120.0/136.5 MB 22.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 121.1/136.5 MB 23.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 122.2/136.5 MB 24.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 123.3/136.5 MB 23.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 124.4/136.5 MB 23.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 125.6/136.5 MB 23.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 126.7/136.5 MB 23.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 127.9/136.5 MB 24.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 129.0/136.5 MB 23.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 130.1/136.5 MB 23.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 131.2/136.5 MB 23.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 132.3/136.5 MB 23.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  133.4/136.5 MB 23.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  134.6/136.5 MB 23.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  135.7/136.5 MB 23.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  136.5/136.5 MB 23.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  136.5/136.5 MB 23.4 MB/s eta 0:00:01\n",
      "   --------------------------------------- 136.5/136.5 MB 19.2 MB/s eta 0:00:00\n",
      "Installing collected packages: bitsandbytes\n",
      "  Attempting uninstall: bitsandbytes\n",
      "    Found existing installation: bitsandbytes 0.43.1\n",
      "    Uninstalling bitsandbytes-0.43.1:\n",
      "      Successfully uninstalled bitsandbytes-0.43.1\n",
      "Successfully installed bitsandbytes-0.43.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: wandb in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (0.17.4)\n",
      "Collecting wandb\n",
      "  Downloading wandb-0.17.9-py3-none-win_amd64.whl.metadata (9.6 kB)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from wandb) (4.2.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<6,>=3.19.0 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from wandb) (4.25.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from wandb) (6.0.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from wandb) (6.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from wandb) (2.32.2)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from wandb) (2.9.0)\n",
      "Requirement already satisfied: setproctitle in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from wandb) (1.3.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from wandb) (69.5.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from click!=8.0.0,>=7.1->wandb) (0.4.6)\n",
      "Requirement already satisfied: six>=1.4.0 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2024.7.4)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
      "Downloading wandb-0.17.9-py3-none-win_amd64.whl (6.6 MB)\n",
      "   ---------------------------------------- 0.0/6.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/6.6 MB 660.6 kB/s eta 0:00:10\n",
      "   - -------------------------------------- 0.2/6.6 MB 1.3 MB/s eta 0:00:05\n",
      "   -- ------------------------------------- 0.4/6.6 MB 2.2 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 0.8/6.6 MB 3.6 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 1.7/6.6 MB 5.9 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 2.4/6.6 MB 7.8 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 2.5/6.6 MB 6.9 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 4.1/6.6 MB 9.7 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 4.1/6.6 MB 9.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.5/6.6 MB 12.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.6/6.6 MB 12.3 MB/s eta 0:00:00\n",
      "Installing collected packages: wandb\n",
      "  Attempting uninstall: wandb\n",
      "    Found existing installation: wandb 0.17.4\n",
      "    Uninstalling wandb-0.17.4:\n",
      "      Successfully uninstalled wandb-0.17.4\n",
      "Successfully installed wandb-0.17.9\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# install all the necessary Python packages.\n",
    "\n",
    "%pip install -U transformers \n",
    "%pip install -U datasets \n",
    "%pip install -U accelerate \n",
    "%pip install -U peft \n",
    "%pip install -U trl \n",
    "%pip install -U bitsandbytes \n",
    "%pip install -U wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary Python pages for loading the dataset, model, and tokenizer and fine-tuning.\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftModel,\n",
    "    prepare_model_for_kbit_training,\n",
    "    get_peft_model,\n",
    ")\n",
    "import os, torch, wandb\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer, setup_chat_format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well be tracking the training process using the Weights & Biases and then saving the fine-tuned model on Hugging Face, and for that, we have to log in to both Hugging Face Hub and Weights & Biases using the API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to C:\\Users\\USER\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:od0qo1im) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fluent-snowflake-5</strong> at: <a href='https://wandb.ai/msfasha/Fine-tune%20Llama%203%208B%20on%20Medical%20Dataset/runs/od0qo1im' target=\"_blank\">https://wandb.ai/msfasha/Fine-tune%20Llama%203%208B%20on%20Medical%20Dataset/runs/od0qo1im</a><br/> View project at: <a href='https://wandb.ai/msfasha/Fine-tune%20Llama%203%208B%20on%20Medical%20Dataset' target=\"_blank\">https://wandb.ai/msfasha/Fine-tune%20Llama%203%208B%20on%20Medical%20Dataset</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240907_183202-od0qo1im\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:od0qo1im). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\USER\\mycode\\llm\\finetune_llm\\wandb\\run-20240907_184419-epttq16j</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/msfasha/Fine-tune%20Llama%203%208B%20on%20Medical%20Dataset/runs/epttq16j' target=\"_blank\">copper-lake-6</a></strong> to <a href='https://wandb.ai/msfasha/Fine-tune%20Llama%203%208B%20on%20Medical%20Dataset' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/msfasha/Fine-tune%20Llama%203%208B%20on%20Medical%20Dataset' target=\"_blank\">https://wandb.ai/msfasha/Fine-tune%20Llama%203%208B%20on%20Medical%20Dataset</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/msfasha/Fine-tune%20Llama%203%208B%20on%20Medical%20Dataset/runs/epttq16j' target=\"_blank\">https://wandb.ai/msfasha/Fine-tune%20Llama%203%208B%20on%20Medical%20Dataset/runs/epttq16j</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The following code was updated using chatgpt so that it does not use kaggle secrets\n",
    "from huggingface_hub import login\n",
    "import json\n",
    "import wandb\n",
    "\n",
    "# Load the tokens from the configuration file\n",
    "with open(\"config.json\", \"r\") as file:\n",
    "    config = json.load(file)\n",
    "\n",
    "hf_token = config[\"HUGGINGFACE_TOKEN\"]\n",
    "wb_token = config[\"WANDB_TOKEN\"]\n",
    "\n",
    "# Login to Hugging Face\n",
    "login(token=hf_token)\n",
    "\n",
    "# Login to Weights & Biases\n",
    "wandb.login(key=wb_token)\n",
    "run = wandb.init(\n",
    "    project='Fine-tune Llama 3 8B on Medical Dataset', \n",
    "    job_type=\"training\", \n",
    "    anonymous=\"allow\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the base model, dataset, and new model variable. \n",
    "Well load the base model and the dataset from the HugginFace Hub and then save the new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_model = \"/kaggle/input/llama-3/transformers/8b-chat-hf/1\"\n",
    "base_model = \"meta-llama/Meta-Llama-3-8B\"\n",
    "dataset_name = \"ruslanmv/ai-medical-chatbot\"\n",
    "new_model = \"llama-3-8b-chat-doctor\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the data type and attention implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_dtype = torch.float16\n",
    "attn_implementation = \"eager\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check GPU readiness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "0\n",
      "NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available())  # Should return True\n",
    "print(torch.cuda.current_device())  # Should return the GPU device index, usually 0\n",
    "print(torch.cuda.get_device_name(0))  # Should return the name of your GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the model and tokenizer\n",
    "\n",
    "In this part, well load the model from huggingface. However, due to memory constraints, were unable to load the full model. Therefore, were loading the model using 4-bit precision.\n",
    "\n",
    "Our goal in this project is to reduce memory usage and speed up the fine-tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|| 4/4 [00:09<00:00,  2.30s/it]\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "if not torch.cuda.is_available():\n",
    "    raise RuntimeError(\"No GPU found. A GPU is needed for quantization.\")\n",
    "\n",
    "# QLoRA config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch_dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=attn_implementation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the tokenizer and then set up a model and tokenizer for conversational AI tasks. By default, it uses the chatml template from OpenAI, which will convert the input text into a chat-like format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "model, tokenizer = setup_chat_format(model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the adapter to the layer\n",
    "Fine-tuning the full model will take a lot of time, so to improve the training time, well attach the adapter layer with a few parameters, making the entire process faster and more memory-efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA config\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n",
    ")\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataset\n",
    "To load and pre-process our dataset, we:\n",
    "\n",
    "1. Load the ruslanmv/ai-medical-chatbot dataset, shuffle it, and select only the top 1000 rows. This will significantly reduce the training time.\n",
    "\n",
    "2. Format the chat template to make it conversational. Combine the patient questions and doctor responses into a \"text\" column.\n",
    "\n",
    "3. Display a sample from the text column (the text column has a chat-like format with special tokens).\n",
    "\n",
    "\n",
    "#Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|| 863/863 [00:00<00:00, 5.71kB/s]\n"
     ]
    }
   ],
   "source": [
    "#Importing the dataset\n",
    "dataset = load_dataset(dataset_name, split=\"all\")\n",
    "dataset = dataset.shuffle(seed=65).select(range(1000)) # Only use 1000 samples for quick demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|| 1000/1000 [00:00<00:00, 17970.53 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<|im_start|>user\\nFell on sidewalk face first about 8 hrs ago. Swollen, cut lip bruised and cut knee, and hurt pride initially. Now have muscle and shoulder pain, stiff jaw(think this is from the really swollen lip),pain in wrist, and headache. I assume this is all normal but are there specific things I should look for or will I just be in pain for a while given the hard fall?<|im_end|>\\n<|im_start|>assistant\\nHello and welcome to HCM,The injuries caused on various body parts have to be managed.The cut and swollen lip has to be managed by sterile dressing.The body pains, pain on injured site and jaw pain should be managed by pain killer and muscle relaxant.I suggest you to consult your primary healthcare provider for clinical assessment.In case there is evidence of infection in any of the injured sites, a course of antibiotics may have to be started to control the infection.Thanks and take careDr Shailja P Wahal<|im_end|>\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_chat_template(row):\n",
    "    row_json = [{\"role\": \"user\", \"content\": row[\"Patient\"]},\n",
    "               {\"role\": \"assistant\", \"content\": row[\"Doctor\"]}]\n",
    "    row[\"text\"] = tokenizer.apply_chat_template(row_json, tokenize=False)\n",
    "    return row\n",
    "\n",
    "dataset = dataset.map(\n",
    "    format_chat_template,\n",
    "    # num_proc=4,\n",
    ")\n",
    "\n",
    "dataset['text'][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Split the dataset into a training and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "We are setting the model hyperparameters so that we can run it on the Kaggle. You can learn about each hyperparameter by reading the Fine-Tuning Llama 2 tutorial.\n",
    "\n",
    "We are fine-tuning the model for one epoch and logging the metrics using the Weights and Biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\envs\\myenv\\Lib\\site-packages\\transformers\\training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of  Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "    output_dir=new_model,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    num_train_epochs=1,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=0.2,\n",
    "    logging_steps=1,\n",
    "    warmup_steps=10,\n",
    "    logging_strategy=\"steps\",\n",
    "    learning_rate=2e-4,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    group_by_length=True,\n",
    "    report_to=\"wandb\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well now set up a supervised fine-tuning (SFT) trainer and provide a train and evaluation dataset, LoRA configuration, training argument, tokenizer, and model. Were keeping the max_seq_length to 512 to avoid exceeding GPU memory during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\envs\\myenv\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length, dataset_text_field. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "c:\\Users\\USER\\anaconda3\\envs\\myenv\\Lib\\site-packages\\trl\\trainer\\sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\USER\\anaconda3\\envs\\myenv\\Lib\\site-packages\\trl\\trainer\\sft_trainer.py:321: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "Map:   0%|          | 0/900 [00:00<?, ? examples/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mSFTTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_text_field\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_arguments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpacking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\myenv\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:101\u001b[0m, in \u001b[0;36m_deprecate_arguments.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     99\u001b[0m         message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m custom_message\n\u001b[0;32m    100\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m)\n\u001b[1;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\myenv\\Lib\\site-packages\\trl\\trainer\\sft_trainer.py:372\u001b[0m, in \u001b[0;36mSFTTrainer.__init__\u001b[1;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics, peft_config, dataset_text_field, packing, formatting_func, max_seq_length, infinite, num_of_sequences, chars_per_token, dataset_num_proc, dataset_batch_size, neftune_noise_alpha, model_init_kwargs, dataset_kwargs, eval_packing)\u001b[0m\n\u001b[0;32m    370\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m PartialState()\u001b[38;5;241m.\u001b[39mlocal_main_process_first():\n\u001b[0;32m    371\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m train_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 372\u001b[0m         train_dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    373\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    374\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    375\u001b[0m \u001b[43m            \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpacking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    376\u001b[0m \u001b[43m            \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_text_field\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    377\u001b[0m \u001b[43m            \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    378\u001b[0m \u001b[43m            \u001b[49m\u001b[43mformatting_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    379\u001b[0m \u001b[43m            \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_of_sequences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    380\u001b[0m \u001b[43m            \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchars_per_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    381\u001b[0m \u001b[43m            \u001b[49m\u001b[43mremove_unused_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremove_unused_columns\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    382\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    384\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m eval_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    385\u001b[0m         _multiple \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(eval_dataset, \u001b[38;5;28mdict\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\myenv\\Lib\\site-packages\\trl\\trainer\\sft_trainer.py:523\u001b[0m, in \u001b[0;36mSFTTrainer._prepare_dataset\u001b[1;34m(self, dataset, tokenizer, packing, dataset_text_field, max_seq_length, formatting_func, num_of_sequences, chars_per_token, remove_unused_columns, append_concat_token, add_special_tokens, skip_prepare_dataset)\u001b[0m\n\u001b[0;32m    520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n\u001b[0;32m    522\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m packing:\n\u001b[1;32m--> 523\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_non_packed_dataloader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset_text_field\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mformatting_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    529\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremove_unused_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    531\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    533\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    534\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_packed_dataloader(\n\u001b[0;32m    535\u001b[0m         tokenizer,\n\u001b[0;32m    536\u001b[0m         dataset,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    543\u001b[0m         add_special_tokens,\n\u001b[0;32m    544\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\myenv\\Lib\\site-packages\\trl\\trainer\\sft_trainer.py:601\u001b[0m, in \u001b[0;36mSFTTrainer._prepare_non_packed_dataloader\u001b[1;34m(self, tokenizer, dataset, dataset_text_field, max_seq_length, formatting_func, add_special_tokens, remove_unused_columns)\u001b[0m\n\u001b[0;32m    599\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataset, datasets\u001b[38;5;241m.\u001b[39mDataset):\n\u001b[0;32m    600\u001b[0m     map_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_proc\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_num_proc  \u001b[38;5;66;03m# this arg is not available for IterableDataset\u001b[39;00m\n\u001b[1;32m--> 601\u001b[0m tokenized_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmap_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    603\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokenized_dataset\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\myenv\\Lib\\site-packages\\datasets\\arrow_dataset.py:602\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    600\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    601\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 602\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    603\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[0;32m    604\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[0;32m    605\u001b[0m     \u001b[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\myenv\\Lib\\site-packages\\datasets\\arrow_dataset.py:567\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    560\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    561\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[0;32m    562\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[0;32m    563\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[0;32m    564\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[0;32m    565\u001b[0m }\n\u001b[0;32m    566\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 567\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    568\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[0;32m    569\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\myenv\\Lib\\site-packages\\datasets\\arrow_dataset.py:3167\u001b[0m, in \u001b[0;36mDataset.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[0;32m   3161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3162\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[0;32m   3163\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3164\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[0;32m   3165\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3166\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m-> 3167\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_single\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdataset_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   3168\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   3169\u001b[0m \u001b[43m                \u001b[49m\u001b[43mshards_done\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\myenv\\Lib\\site-packages\\datasets\\arrow_dataset.py:3558\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[1;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[0;32m   3554\u001b[0m indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[0;32m   3555\u001b[0m     \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mslice\u001b[39m(i, i \u001b[38;5;241m+\u001b[39m batch_size)\u001b[38;5;241m.\u001b[39mindices(shard\u001b[38;5;241m.\u001b[39mnum_rows)))\n\u001b[0;32m   3556\u001b[0m )  \u001b[38;5;66;03m# Something simpler?\u001b[39;00m\n\u001b[0;32m   3557\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3558\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[43mapply_function_on_filtered_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3559\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3560\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3561\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_same_num_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mshard\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_indexes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3562\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3563\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3564\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m NumExamplesMismatchError:\n\u001b[0;32m   3565\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetTransformationNotAllowedError(\n\u001b[0;32m   3566\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt create or remove existing examples. You can first run `.drop_index() to remove your index and then re-add it.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3567\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\myenv\\Lib\\site-packages\\datasets\\arrow_dataset.py:3427\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[1;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[0;32m   3425\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_rank:\n\u001b[0;32m   3426\u001b[0m     additional_args \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (rank,)\n\u001b[1;32m-> 3427\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3428\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[0;32m   3429\u001b[0m     processed_inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   3430\u001b[0m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mkeys_to_format\n\u001b[0;32m   3431\u001b[0m     }\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\myenv\\Lib\\site-packages\\trl\\trainer\\sft_trainer.py:562\u001b[0m, in \u001b[0;36mSFTTrainer._prepare_non_packed_dataloader.<locals>.tokenize\u001b[1;34m(element)\u001b[0m\n\u001b[0;32m    560\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize\u001b[39m(element):\n\u001b[0;32m    561\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m tokenizer(\n\u001b[1;32m--> 562\u001b[0m         \u001b[43melement\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdataset_text_field\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m use_formatting_func \u001b[38;5;28;01melse\u001b[39;00m formatting_func(element),\n\u001b[0;32m    563\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m    564\u001b[0m         truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    565\u001b[0m         padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    566\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mmax_seq_length,\n\u001b[0;32m    567\u001b[0m         return_overflowing_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    568\u001b[0m         return_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    569\u001b[0m     )\n\u001b[0;32m    571\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_formatting_func \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_sanity_checked:\n\u001b[0;32m    572\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(formatting_func(element), \u001b[38;5;28mlist\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\myenv\\Lib\\site-packages\\datasets\\formatting\\formatting.py:277\u001b[0m, in \u001b[0;36mLazyDict.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[1;32m--> 277\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeys_to_format:\n\u001b[0;32m    279\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'text'"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=512,\n",
    "    dataset_text_field=\"text\",\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing= False,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
