{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on code from: \n",
    "https://github.com/matteo-stat/transformers-llm-llama3.1-fine-tuning-qlora/blob/main/01-llama31-qlora-fine-tuning.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from unsloth import FastLanguageModel\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "# ----------------------------------------------------------------------------------------\n",
    "# max sequence length that the model will receive as input\n",
    "max_seq_length = 2048 \n",
    "\n",
    "# leave to none for auto detection (otherwise can be float16 or blfloat16, depending on your gpu)\n",
    "dtype = None\n",
    "\n",
    "# use 4 bit quantization to reduce memory usage\n",
    "load_in_4bit = True\n",
    "\n",
    "# seed for reproducibility\n",
    "seed = 1993\n",
    "\n",
    "# output path for lora adapters\n",
    "output_path_adapters = 'models/llama31-8b-qlora-adapters-only'\n",
    "output_path_gguf = 'models/llama31-8b-qlora-gguf'\n",
    "# ----------------------------------------------------------------------------------------\n",
    "\n",
    "# load dataset\n",
    "dataset = load_dataset(\n",
    "    'parquet',\n",
    "    data_files={\n",
    "        'train': f'data/chat-samples-en-it.parquet',\n",
    "    }\n",
    ")\n",
    "\n",
    "# load model and tokenizer\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name='unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit',\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")\n",
    "\n",
    "# add lora adapters to the model, below some notes\n",
    "# --- r (lora matrix rank) ---\n",
    "# higher ranks can store more information but increase computational load and memory cost\n",
    "# typical rank values are between 8 and 256 (suggested value 8, 16, 32, 64, 128)\n",
    "# try to start with lower values and then increase if needed\n",
    "# --- lora_alpha (alpha) ---\n",
    "# scaling factor for updates that impact the lora adapters contribution to the model output\n",
    "# typical values are 1x or 2x the rank value\n",
    "# --- target modules ---\n",
    "# lora can be applied to various model layers\n",
    "# if more layers are choosen then memory cost increase as the number of trainable parameters\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model=model,\n",
    "    r=16,\n",
    "    target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj'],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0, # 0 is optimized with unsloth\n",
    "    bias='none',    # 'none' is optimized with unsloth\n",
    "    use_gradient_checkpointing='unsloth', # use 'unsloth' to reduce vram usage\n",
    "    random_state=seed,\n",
    "    max_seq_length=max_seq_length,\n",
    "    use_rslora=False,  # rank stabilized lora\n",
    ")\n",
    "\n",
    "# training arguments\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=5,\n",
    "    num_train_epochs=2,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=not is_bfloat16_supported(),\n",
    "    bf16=is_bfloat16_supported(),\n",
    "    logging_steps=1,\n",
    "    optim='adamw_8bit',\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type='linear',\n",
    "    seed=seed,\n",
    "    output_dir='checkpoints/llama31-8b-qlora-fine-tuned',\n",
    ")\n",
    "\n",
    "# function for apply a chat template to conversations in the dataset\n",
    "# \"information\" column contains a json providing context about question for helping the llm generate a proper answer\n",
    "# if you don't have the \"information\" column simply remove it from the function below\n",
    "def apply_chat_template(examples):\n",
    "    chat_samples = []\n",
    "    for question, information, answer in zip(examples['question'], examples['information'], examples['answer']):\n",
    "        chat = [\n",
    "            {'role': 'system', 'content': 'you are a helpful assistant'},\n",
    "            {'role': 'user', 'content': question},\n",
    "            {'role': 'ipython', 'content': information},\n",
    "            {'role': 'assistant', 'content': answer}\n",
    "        ]\n",
    "        chat_sample = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=False)\n",
    "        chat_samples.append(chat_sample)\n",
    "\n",
    "    return {'text': chat_samples}\n",
    "\n",
    "# apply a chat template to conversations in the dataset\n",
    "dataset = dataset.map(apply_chat_template, batched=True)\n",
    "\n",
    "# trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset['train'],\n",
    "    dataset_text_field='text',\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    packing=False, # can make training 5x faster for short sequences\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "# start training\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "# save only lora adapters weights\n",
    "model.save_pretrained(output_path_adapters)\n",
    "tokenizer.save_pretrained(output_path_adapters)\n",
    "\n",
    "# alternative method for saving only lora adapters\n",
    "# model.save_pretrained_merged(output_path_adapters, tokenizer, save_method='lora')\n",
    "\n",
    "# save quantized versions, useful for using it in other frameworks like llama.cpp\n",
    "# WARNING: this is gonna merge and quantize also the lora adapters layers, reducing their effectiveness\n",
    "# be aware of this and expect worse results compared to the model with lora adapters at full precision\n",
    "model.save_pretrained_gguf(output_path_gguf, tokenizer, quantization_method='q5_k_m')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
