{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edd4c34a",
   "metadata": {},
   "source": [
    "# Llama 3.1 Training on Arabic Wikipedia Dataset (Updated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48ce8c3",
   "metadata": {},
   "source": [
    "This notebook was created by ChatGPT.<br>\n",
    "The motivation for this notebook came from the Unsloth version, but the problems I faced on Colab motivated me to create a version without unsloth.<br>\n",
    "During my work on unsloth version i faced two problems:\n",
    "- Colab was slow, everytime i needed to download everything again and again, and lots of versioning conflicts.\n",
    "- Locally installing unsloth on windows was not possible because it required a library called triton was could not be installed on windows (only linux)\n",
    "\n",
    "How did I get here:\n",
    "\n",
    "https://www.reddit.com/r/LocalLLaMA/comments/1d85h3l/fine_tuning_llama_3_on_dhivehi_%DE%8B%DE%88%DE%80_language/ \n",
    "\n",
    "https://www.reddit.com/r/LocalLLaMA/comments/1d86k5y/continued_pretraining_2x_faster_notebook_to/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button\n",
    "\n",
    "https://colab.research.google.com/drive/1tEd1FrOXWMnCU9UIvdYhs61tkxdMuKZu?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42921f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (4.44.2)\n",
      "Requirement already satisfied: datasets in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (3.0.0)\n",
      "Requirement already satisfied: accelerate in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (0.34.2)\n",
      "Requirement already satisfied: peft in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (0.12.0)\n",
      "Requirement already satisfied: bitsandbytes in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (0.43.3)\n",
      "Requirement already satisfied: torch in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (2.4.1+cu121)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement triton (from versions: none)\n",
      "ERROR: No matching distribution found for triton\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xformers==0.0.28.post1\n",
      "  Using cached xformers-0.0.28.post1.tar.gz (7.8 MB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: torch>=2.4 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from xformers==0.0.28.post1) (2.4.1+cu121)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from xformers==0.0.28.post1) (2.0.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch>=2.4->xformers==0.0.28.post1) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch>=2.4->xformers==0.0.28.post1) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch>=2.4->xformers==0.0.28.post1) (1.13.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch>=2.4->xformers==0.0.28.post1) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch>=2.4->xformers==0.0.28.post1) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch>=2.4->xformers==0.0.28.post1) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch>=2.4->xformers==0.0.28.post1) (72.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from jinja2->torch>=2.4->xformers==0.0.28.post1) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from sympy->torch>=2.4->xformers==0.0.28.post1) (1.3.0)\n",
      "Building wheels for collected packages: xformers\n",
      "  Building wheel for xformers (setup.py): started\n",
      "  Building wheel for xformers (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for xformers\n",
      "Failed to build xformers\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × python setup.py bdist_wheel did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [218 lines of output]\n",
      "      fatal: not a git repository (or any of the parent directories): .git\n",
      "      running bdist_wheel\n",
      "      running build\n",
      "      running build_py\n",
      "      creating build\n",
      "      creating build\\lib.win-amd64-cpython-312\n",
      "      creating build\\lib.win-amd64-cpython-312\\xformers\n",
      "      copying xformers\\attn_bias_utils.py -> build\\lib.win-amd64-cpython-312\\xformers\n",
      "      copying xformers\\checkpoint.py -> build\\lib.win-amd64-cpython-312\\xformers\n",
      "      copying xformers\\info.py -> build\\lib.win-amd64-cpython-312\\xformers\n",
      "      copying xformers\\test.py -> build\\lib.win-amd64-cpython-312\\xformers\n",
      "      copying xformers\\utils.py -> build\\lib.win-amd64-cpython-312\\xformers\n",
      "      copying xformers\\_cpp_lib.py -> build\\lib.win-amd64-cpython-312\\xformers\n",
      "      copying xformers\\_deprecation_warning.py -> build\\lib.win-amd64-cpython-312\\xformers\n",
      "      copying xformers\\__init__.py -> build\\lib.win-amd64-cpython-312\\xformers\n",
      "      creating build\\lib.win-amd64-cpython-312\\xformers\\benchmarks\n",
      "      copying xformers\\benchmarks\\benchmark_attn_decoding.py -> build\\lib.win-amd64-cpython-312\\xformers\\benchmarks\n",
      "      copying xformers\\benchmarks\\benchmark_core.py -> build\\lib.win-amd64-cpython-312\\xformers\\benchmarks\n",
      "      copying xformers\\benchmarks\\benchmark_indexing.py -> build\\lib.win-amd64-cpython-312\\xformers\\benchmarks\n",
      "      copying xformers\\benchmarks\\benchmark_mem_eff_attention.py -> build\\lib.win-amd64-cpython-312\\xformers\\benchmarks\n",
      "      copying xformers\\benchmarks\\benchmark_merge_attentions.py -> build\\lib.win-amd64-cpython-312\\xformers\\benchmarks\n",
      "      copying xformers\\benchmarks\\benchmark_multi_head_dispatch.py -> build\\lib.win-amd64-cpython-312\\xformers\\benchmarks\n",
      "      copying xformers\\benchmarks\\benchmark_nystrom_utils.py -> build\\lib.win-amd64-cpython-312\\xformers\\benchmarks\n",
      "      copying xformers\\benchmarks\\benchmark_revnet.py -> build\\lib.win-amd64-cpython-312\\xformers\\benchmarks\n",
      "      copying xformers\\benchmarks\\benchmark_sddmm.py -> build\\lib.win-amd64-cpython-312\\xformers\\benchmarks\n",
      "      copying xformers\\benchmarks\\benchmark_sequence_parallel_fused.py -> build\\lib.win-amd64-cpython-312\\xformers\\benchmarks\n",
      "      copying xformers\\benchmarks\\benchmark_sp24.py -> build\\lib.win-amd64-cpython-312\\xformers\\benchmarks\n",
      "      copying xformers\\benchmarks\\benchmark_swiglu.py -> build\\lib.win-amd64-cpython-312\\xformers\\benchmarks\n",
      "      copying xformers\\benchmarks\\benchmark_tiled_matmul.py -> build\\lib.win-amd64-cpython-312\\xformers\\benchmarks\n",
      "      copying xformers\\benchmarks\\utils.py -> build\\lib.win-amd64-cpython-312\\xformers\\benchmarks\n",
      "      copying xformers\\benchmarks\\__init__.py -> build\\lib.win-amd64-cpython-312\\xformers\\benchmarks\n",
      "      creating build\\lib.win-amd64-cpython-312\\xformers\\components\n",
      "      copying xformers\\components\\activations.py -> build\\lib.win-amd64-cpython-312\\xformers\\components\n",
      "      copying xformers\\components\\input_projection.py -> build\\lib.win-amd64-cpython-312\\xformers\\components\n",
      "      copying xformers\\components\\multi_head_dispatch.py -> build\\lib.win-amd64-cpython-312\\xformers\\components\n",
      "      copying xformers\\components\\patch_embedding.py -> build\\lib.win-amd64-cpython-312\\xformers\\components\n",
      "      copying xformers\\components\\residual.py -> build\\lib.win-amd64-cpython-312\\xformers\\components\n",
      "      copying xformers\\components\\reversible.py -> build\\lib.win-amd64-cpython-312\\xformers\\components\n",
      "      copying xformers\\components\\simplicial_embedding.py -> build\\lib.win-amd64-cpython-312\\xformers\\components\n",
      "      copying xformers\\components\\__init__.py -> build\\lib.win-amd64-cpython-312\\xformers\\components\n",
      "      creating build\\lib.win-amd64-cpython-312\\xformers\\factory\n",
      "      copying xformers\\factory\\block_configs.py -> build\\lib.win-amd64-cpython-312\\xformers\\factory\n",
      "      copying xformers\\factory\\block_factory.py -> build\\lib.win-amd64-cpython-312\\xformers\\factory\n",
      "      copying xformers\\factory\\hydra_helper.py -> build\\lib.win-amd64-cpython-312\\xformers\\factory\n",
      "      copying xformers\\factory\\model_factory.py -> build\\lib.win-amd64-cpython-312\\xformers\\factory\n",
      "      copying xformers\\factory\\weight_init.py -> build\\lib.win-amd64-cpython-312\\xformers\\factory\n",
      "      copying xformers\\factory\\__init__.py -> build\\lib.win-amd64-cpython-312\\xformers\\factory\n",
      "      creating build\\lib.win-amd64-cpython-312\\xformers\\helpers\n",
      "      copying xformers\\helpers\\hierarchical_configs.py -> build\\lib.win-amd64-cpython-312\\xformers\\helpers\n",
      "      copying xformers\\helpers\\test_utils.py -> build\\lib.win-amd64-cpython-312\\xformers\\helpers\n",
      "      copying xformers\\helpers\\timm_sparse_attention.py -> build\\lib.win-amd64-cpython-312\\xformers\\helpers\n",
      "      copying xformers\\helpers\\__init__.py -> build\\lib.win-amd64-cpython-312\\xformers\\helpers\n",
      "      creating build\\lib.win-amd64-cpython-312\\xformers\\ops\n",
      "      copying xformers\\ops\\common.py -> build\\lib.win-amd64-cpython-312\\xformers\\ops\n",
      "      copying xformers\\ops\\differentiable_collectives.py -> build\\lib.win-amd64-cpython-312\\xformers\\ops\n",
      "      copying xformers\\ops\\indexing.py -> build\\lib.win-amd64-cpython-312\\xformers\\ops\n",
      "      copying xformers\\ops\\ipc.py -> build\\lib.win-amd64-cpython-312\\xformers\\ops\n",
      "      copying xformers\\ops\\modpar_layers.py -> build\\lib.win-amd64-cpython-312\\xformers\\ops\n",
      "      copying xformers\\ops\\rmsnorm.py -> build\\lib.win-amd64-cpython-312\\xformers\\ops\n",
      "      copying xformers\\ops\\rope_padded.py -> build\\lib.win-amd64-cpython-312\\xformers\\ops\n",
      "      copying xformers\\ops\\seqpar.py -> build\\lib.win-amd64-cpython-312\\xformers\\ops\n",
      "      copying xformers\\ops\\sequence_parallel_fused_ops.py -> build\\lib.win-amd64-cpython-312\\xformers\\ops\n",
      "      copying xformers\\ops\\sp24.py -> build\\lib.win-amd64-cpython-312\\xformers\\ops\n",
      "      copying xformers\\ops\\swiglu_op.py -> build\\lib.win-amd64-cpython-312\\xformers\\ops\n",
      "      copying xformers\\ops\\tiled_matmul.py -> build\\lib.win-amd64-cpython-312\\xformers\\ops\n",
      "      copying xformers\\ops\\unbind.py -> build\\lib.win-amd64-cpython-312\\xformers\\ops\n",
      "      copying xformers\\ops\\__init__.py -> build\\lib.win-amd64-cpython-312\\xformers\\ops\n",
      "      creating build\\lib.win-amd64-cpython-312\\xformers\\profiler\n",
      "      copying xformers\\profiler\\api.py -> build\\lib.win-amd64-cpython-312\\xformers\\profiler\n",
      "      copying xformers\\profiler\\device_limits.py -> build\\lib.win-amd64-cpython-312\\xformers\\profiler\n",
      "      copying xformers\\profiler\\find_slowest.py -> build\\lib.win-amd64-cpython-312\\xformers\\profiler\n",
      "      copying xformers\\profiler\\profiler.py -> build\\lib.win-amd64-cpython-312\\xformers\\profiler\n",
      "      copying xformers\\profiler\\profiler_dcgm.py -> build\\lib.win-amd64-cpython-312\\xformers\\profiler\n",
      "      copying xformers\\profiler\\profiler_dcgm_impl.py -> build\\lib.win-amd64-cpython-312\\xformers\\profiler\n",
      "      copying xformers\\profiler\\profile_analyzer.py -> build\\lib.win-amd64-cpython-312\\xformers\\profiler\n",
      "      copying xformers\\profiler\\__init__.py -> build\\lib.win-amd64-cpython-312\\xformers\\profiler\n",
      "      creating build\\lib.win-amd64-cpython-312\\xformers\\sparse\n",
      "      copying xformers\\sparse\\blocksparse_tensor.py -> build\\lib.win-amd64-cpython-312\\xformers\\sparse\n",
      "      copying xformers\\sparse\\csr_tensor.py -> build\\lib.win-amd64-cpython-312\\xformers\\sparse\n",
      "      copying xformers\\sparse\\utils.py -> build\\lib.win-amd64-cpython-312\\xformers\\sparse\n",
      "      copying xformers\\sparse\\_csr_ops.py -> build\\lib.win-amd64-cpython-312\\xformers\\sparse\n",
      "      copying xformers\\sparse\\__init__.py -> build\\lib.win-amd64-cpython-312\\xformers\\sparse\n",
      "      creating build\\lib.win-amd64-cpython-312\\xformers\\triton\n",
      "      copying xformers\\triton\\vararg_kernel.py -> build\\lib.win-amd64-cpython-312\\xformers\\triton\n",
      "      copying xformers\\triton\\__init__.py -> build\\lib.win-amd64-cpython-312\\xformers\\triton\n",
      "      creating build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\n",
      "      copying xformers\\_flash_attn\\bert_padding.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\n",
      "      copying xformers\\_flash_attn\\flash_attn_interface.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\n",
      "      copying xformers\\_flash_attn\\flash_attn_triton.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\n",
      "      copying xformers\\_flash_attn\\flash_attn_triton_og.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\n",
      "      copying xformers\\_flash_attn\\flash_blocksparse_attention.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\n",
      "      copying xformers\\_flash_attn\\flash_blocksparse_attn_interface.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\n",
      "      copying xformers\\_flash_attn\\fused_softmax.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\n",
      "      copying xformers\\_flash_attn\\__init__.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\n",
      "      creating build\\lib.win-amd64-cpython-312\\xformers\\benchmarks\\LRA\n",
      "      copying xformers\\benchmarks\\LRA\\batch_fetch_results.py -> build\\lib.win-amd64-cpython-312\\xformers\\benchmarks\\LRA\n",
      "      copying xformers\\benchmarks\\LRA\\batch_submit.py -> build\\lib.win-amd64-cpython-312\\xformers\\benchmarks\\LRA\n",
      "      copying xformers\\benchmarks\\LRA\\run_grid_search.py -> build\\lib.win-amd64-cpython-312\\xformers\\benchmarks\\LRA\n",
      "      copying xformers\\benchmarks\\LRA\\run_tasks.py -> build\\lib.win-amd64-cpython-312\\xformers\\benchmarks\\LRA\n",
      "      copying xformers\\benchmarks\\LRA\\run_with_submitit.py -> build\\lib.win-amd64-cpython-312\\xformers\\benchmarks\\LRA\n",
      "      copying xformers\\benchmarks\\LRA\\__init__.py -> build\\lib.win-amd64-cpython-312\\xformers\\benchmarks\\LRA\n",
      "      creating build\\lib.win-amd64-cpython-312\\xformers\\benchmarks\\LRA\\code\n",
      "      copying xformers\\benchmarks\\LRA\\code\\dataset.py -> build\\lib.win-amd64-cpython-312\\xformers\\benchmarks\\LRA\\code\n",
      "      copying xformers\\benchmarks\\LRA\\code\\model_wrapper.py -> build\\lib.win-amd64-cpython-312\\xformers\\benchmarks\\LRA\\code\n",
      "      copying xformers\\benchmarks\\LRA\\code\\__init__.py -> build\\lib.win-amd64-cpython-312\\xformers\\benchmarks\\LRA\\code\n",
      "      creating build\\lib.win-amd64-cpython-312\\xformers\\components\\attention\n",
      "      copying xformers\\components\\attention\\attention_mask.py -> build\\lib.win-amd64-cpython-312\\xformers\\components\\attention\n",
      "      copying xformers\\components\\attention\\attention_patterns.py -> build\\lib.win-amd64-cpython-312\\xformers\\components\\attention\n",
      "      copying xformers\\components\\attention\\base.py -> build\\lib.win-amd64-cpython-312\\xformers\\components\\attention\n",
      "      copying xformers\\components\\attention\\compositional.py -> build\\lib.win-amd64-cpython-312\\xformers\\components\\attention\n",
      "      copying xformers\\components\\attention\\core.py -> build\\lib.win-amd64-cpython-312\\xformers\\components\\attention\n",
      "      copying xformers\\components\\attention\\favor.py -> build\\lib.win-amd64-cpython-312\\xformers\\components\\attention\n",
      "      copying xformers\\components\\attention\\fourier_mix.py -> build\\lib.win-amd64-cpython-312\\xformers\\components\\attention\n",
      "      copying xformers\\components\\attention\\global_tokens.py -> build\\lib.win-amd64-cpython-312\\xformers\\components\\attention\n",
      "      copying xformers\\components\\attention\\lambda_layer.py -> build\\lib.win-amd64-cpython-312\\xformers\\components\\attention\n",
      "      copying xformers\\components\\attention\\linformer.py -> build\\lib.win-amd64-cpython-312\\xformers\\components\\attention\n",
      "      copying xformers\\components\\attention\\local.py -> build\\lib.win-amd64-cpython-312\\xformers\\components\\attention\n",
      "      copying xformers\\components\\attention\\nystrom.py -> build\\lib.win-amd64-cpython-312\\xformers\\components\\attention\n",
      "      copying xformers\\components\\attention\\ortho.py -> build\\lib.win-amd64-cpython-312\\xformers\\components\\attention\n",
      "      copying xformers\\components\\attention\\pooling.py -> build\\lib.win-amd64-cpython-312\\xformers\\components\\attention\n",
      "      copying xformers\\components\\attention\\random.py -> build\\lib.win-amd64-cpython-312\\xformers\\components\\attention\n",
      "      copying xformers\\components\\attention\\scaled_dot_product.py -> build\\lib.win-amd64-cpython-312\\xformers\\components\\attention\n",
      "      copying xformers\\components\\attention\\sparsity_config.py -> build\\lib.win-amd64-cpython-312\\xformers\\components\\attention\n",
      "      copying xformers\\components\\attention\\utils.py -> build\\lib.win-amd64-cpython-312\\xformers\\components\\attention\n",
      "      copying xformers\\components\\attention\\visual.py -> build\\lib.win-amd64-cpython-312\\xformers\\components\\attention\n",
      "      copying xformers\\components\\attention\\_sputnik_sparse.py -> build\\lib.win-amd64-cpython-312\\xformers\\components\\attention\n",
      "      copying xformers\\components\\attention\\__init__.py -> build\\lib.win-amd64-cpython-312\\xformers\\components\\attention\n",
      "      creating build\\lib.win-amd64-cpython-312\\xformers\\components\\feedforward\n",
      "      copying xformers\\components\\feedforward\\base.py -> build\\lib.win-amd64-cpython-312\\xformers\\components\\feedforward\n",
      "      copying xformers\\components\\feedforward\\conv_mlp.py -> build\\lib.win-amd64-cpython-312\\xformers\\components\\feedforward\n",
      "      copying xformers\\components\\feedforward\\mixture_of_experts.py -> build\\lib.win-amd64-cpython-312\\xformers\\components\\feedforward\n",
      "      copying xformers\\components\\feedforward\\mlp.py -> build\\lib.win-amd64-cpython-312\\xformers\\components\\feedforward\n",
      "      copying xformers\\components\\feedforward\\__init__.py -> build\\lib.win-amd64-cpython-312\\xformers\\components\\feedforward\n",
      "      creating build\\lib.win-amd64-cpython-312\\xformers\\components\\positional_embedding\n",
      "      copying xformers\\components\\positional_embedding\\base.py -> build\\lib.win-amd64-cpython-312\\xformers\\components\\positional_embedding\n",
      "      copying xformers\\components\\positional_embedding\\param.py -> build\\lib.win-amd64-cpython-312\\xformers\\components\\positional_embedding\n",
      "      copying xformers\\components\\positional_embedding\\rotary.py -> build\\lib.win-amd64-cpython-312\\xformers\\components\\positional_embedding\n",
      "      copying xformers\\components\\positional_embedding\\sine.py -> build\\lib.win-amd64-cpython-312\\xformers\\components\\positional_embedding\n",
      "      copying xformers\\components\\positional_embedding\\vocab.py -> build\\lib.win-amd64-cpython-312\\xformers\\components\\positional_embedding\n",
      "      copying xformers\\components\\positional_embedding\\__init__.py -> build\\lib.win-amd64-cpython-312\\xformers\\components\\positional_embedding\n",
      "      creating build\\lib.win-amd64-cpython-312\\xformers\\components\\attention\\feature_maps\n",
      "      copying xformers\\components\\attention\\feature_maps\\base.py -> build\\lib.win-amd64-cpython-312\\xformers\\components\\attention\\feature_maps\n",
      "      copying xformers\\components\\attention\\feature_maps\\softmax.py -> build\\lib.win-amd64-cpython-312\\xformers\\components\\attention\\feature_maps\n",
      "      copying xformers\\components\\attention\\feature_maps\\__init__.py -> build\\lib.win-amd64-cpython-312\\xformers\\components\\attention\\feature_maps\n",
      "      creating build\\lib.win-amd64-cpython-312\\xformers\\ops\\fmha\n",
      "      copying xformers\\ops\\fmha\\attn_bias.py -> build\\lib.win-amd64-cpython-312\\xformers\\ops\\fmha\n",
      "      copying xformers\\ops\\fmha\\ck.py -> build\\lib.win-amd64-cpython-312\\xformers\\ops\\fmha\n",
      "      copying xformers\\ops\\fmha\\ck_decoder.py -> build\\lib.win-amd64-cpython-312\\xformers\\ops\\fmha\n",
      "      copying xformers\\ops\\fmha\\ck_splitk.py -> build\\lib.win-amd64-cpython-312\\xformers\\ops\\fmha\n",
      "      copying xformers\\ops\\fmha\\common.py -> build\\lib.win-amd64-cpython-312\\xformers\\ops\\fmha\n",
      "      copying xformers\\ops\\fmha\\cutlass.py -> build\\lib.win-amd64-cpython-312\\xformers\\ops\\fmha\n",
      "      copying xformers\\ops\\fmha\\dispatch.py -> build\\lib.win-amd64-cpython-312\\xformers\\ops\\fmha\n",
      "      copying xformers\\ops\\fmha\\flash.py -> build\\lib.win-amd64-cpython-312\\xformers\\ops\\fmha\n",
      "      copying xformers\\ops\\fmha\\flash3.py -> build\\lib.win-amd64-cpython-312\\xformers\\ops\\fmha\n",
      "      copying xformers\\ops\\fmha\\torch_attention_compat.py -> build\\lib.win-amd64-cpython-312\\xformers\\ops\\fmha\n",
      "      copying xformers\\ops\\fmha\\triton_splitk.py -> build\\lib.win-amd64-cpython-312\\xformers\\ops\\fmha\n",
      "      copying xformers\\ops\\fmha\\__init__.py -> build\\lib.win-amd64-cpython-312\\xformers\\ops\\fmha\n",
      "      creating build\\lib.win-amd64-cpython-312\\xformers\\ops\\_triton\n",
      "      copying xformers\\ops\\_triton\\k_index_select_cat.py -> build\\lib.win-amd64-cpython-312\\xformers\\ops\\_triton\n",
      "      copying xformers\\ops\\_triton\\k_scaled_index_add.py -> build\\lib.win-amd64-cpython-312\\xformers\\ops\\_triton\n",
      "      copying xformers\\ops\\_triton\\rmsnorm_kernels.py -> build\\lib.win-amd64-cpython-312\\xformers\\ops\\_triton\n",
      "      copying xformers\\ops\\_triton\\rope_padded_kernels.py -> build\\lib.win-amd64-cpython-312\\xformers\\ops\\_triton\n",
      "      copying xformers\\ops\\_triton\\sequence_parallel_fused_kernels.py -> build\\lib.win-amd64-cpython-312\\xformers\\ops\\_triton\n",
      "      copying xformers\\ops\\_triton\\tiled_matmul_kernels.py -> build\\lib.win-amd64-cpython-312\\xformers\\ops\\_triton\n",
      "      copying xformers\\ops\\_triton\\__init__.py -> build\\lib.win-amd64-cpython-312\\xformers\\ops\\_triton\n",
      "      creating build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\\layers\n",
      "      copying xformers\\_flash_attn\\layers\\patch_embed.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\\layers\n",
      "      copying xformers\\_flash_attn\\layers\\rotary.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\\layers\n",
      "      copying xformers\\_flash_attn\\layers\\__init__.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\\layers\n",
      "      creating build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\\losses\n",
      "      copying xformers\\_flash_attn\\losses\\cross_entropy.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\\losses\n",
      "      copying xformers\\_flash_attn\\losses\\__init__.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\\losses\n",
      "      creating build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\\models\n",
      "      copying xformers\\_flash_attn\\models\\baichuan.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\\models\n",
      "      copying xformers\\_flash_attn\\models\\bert.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\\models\n",
      "      copying xformers\\_flash_attn\\models\\bigcode.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\\models\n",
      "      copying xformers\\_flash_attn\\models\\btlm.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\\models\n",
      "      copying xformers\\_flash_attn\\models\\falcon.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\\models\n",
      "      copying xformers\\_flash_attn\\models\\gpt.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\\models\n",
      "      copying xformers\\_flash_attn\\models\\gptj.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\\models\n",
      "      copying xformers\\_flash_attn\\models\\gpt_neox.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\\models\n",
      "      copying xformers\\_flash_attn\\models\\llama.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\\models\n",
      "      copying xformers\\_flash_attn\\models\\opt.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\\models\n",
      "      copying xformers\\_flash_attn\\models\\vit.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\\models\n",
      "      copying xformers\\_flash_attn\\models\\__init__.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\\models\n",
      "      creating build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\\modules\n",
      "      copying xformers\\_flash_attn\\modules\\block.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\\modules\n",
      "      copying xformers\\_flash_attn\\modules\\embedding.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\\modules\n",
      "      copying xformers\\_flash_attn\\modules\\mha.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\\modules\n",
      "      copying xformers\\_flash_attn\\modules\\mlp.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\\modules\n",
      "      copying xformers\\_flash_attn\\modules\\__init__.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\\modules\n",
      "      creating build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\\ops\n",
      "      copying xformers\\_flash_attn\\ops\\activations.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\\ops\n",
      "      copying xformers\\_flash_attn\\ops\\fused_dense.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\\ops\n",
      "      copying xformers\\_flash_attn\\ops\\layer_norm.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\\ops\n",
      "      copying xformers\\_flash_attn\\ops\\rms_norm.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\\ops\n",
      "      copying xformers\\_flash_attn\\ops\\__init__.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\\ops\n",
      "      creating build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\\utils\n",
      "      copying xformers\\_flash_attn\\utils\\benchmark.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\\utils\n",
      "      copying xformers\\_flash_attn\\utils\\distributed.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\\utils\n",
      "      copying xformers\\_flash_attn\\utils\\generation.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\\utils\n",
      "      copying xformers\\_flash_attn\\utils\\pretrained.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\\utils\n",
      "      copying xformers\\_flash_attn\\utils\\__init__.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\\utils\n",
      "      creating build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\\ops\\triton\n",
      "      copying xformers\\_flash_attn\\ops\\triton\\cross_entropy.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\\ops\\triton\n",
      "      copying xformers\\_flash_attn\\ops\\triton\\k_activations.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\\ops\\triton\n",
      "      copying xformers\\_flash_attn\\ops\\triton\\layer_norm.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\\ops\\triton\n",
      "      copying xformers\\_flash_attn\\ops\\triton\\linear.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\\ops\\triton\n",
      "      copying xformers\\_flash_attn\\ops\\triton\\mlp.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\\ops\\triton\n",
      "      copying xformers\\_flash_attn\\ops\\triton\\rotary.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\\ops\\triton\n",
      "      copying xformers\\_flash_attn\\ops\\triton\\__init__.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\\ops\\triton\n",
      "      running build_ext\n",
      "      c:\\Users\\USER\\anaconda3\\envs\\myenv\\Lib\\site-packages\\torch\\utils\\cpp_extension.py:380: UserWarning: Error checking compiler version for cl: [WinError 2] The system cannot find the file specified\n",
      "        warnings.warn(f'Error checking compiler version for {compiler}: {error}')\n",
      "      c:\\Users\\USER\\anaconda3\\envs\\myenv\\Lib\\site-packages\\torch\\utils\\cpp_extension.py:414: UserWarning: The detected CUDA version (12.6) has a minor version mismatch with the version that was used to compile PyTorch (12.1). Most likely this shouldn't be a problem.\n",
      "        warnings.warn(CUDA_MISMATCH_WARN.format(cuda_str_version, torch.version.cuda))\n",
      "      building 'xformers._C_flashattention' extension\n",
      "      error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for xformers\n",
      "ERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (xformers)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pip install transformers datasets accelerate peft bitsandbytes torch triton\n",
    "%pip install xformers==0.0.28.post1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38986687",
   "metadata": {},
   "source": [
    "## Model Loading Using Hugging Face AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85c775ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 8,030,261,248\n",
      "Estimated model size: 14.96 GB\n",
      "Layer: model.embed_tokens.weight | Shape: torch.Size([128256, 4096])\n",
      "Layer: model.layers.0.self_attn.q_proj.weight | Shape: torch.Size([4096, 4096])\n",
      "Layer: model.layers.0.self_attn.k_proj.weight | Shape: torch.Size([1024, 4096])\n",
      "Layer: model.layers.0.self_attn.v_proj.weight | Shape: torch.Size([1024, 4096])\n",
      "Layer: model.layers.0.self_attn.o_proj.weight | Shape: torch.Size([4096, 4096])\n",
      "Layer: model.layers.0.mlp.gate_proj.weight | Shape: torch.Size([14336, 4096])\n",
      "Layer: model.layers.0.mlp.up_proj.weight | Shape: torch.Size([14336, 4096])\n",
      "Layer: model.layers.0.mlp.down_proj.weight | Shape: torch.Size([4096, 14336])\n",
      "Layer: model.layers.0.input_layernorm.weight | Shape: torch.Size([4096])\n",
      "Layer: model.layers.0.post_attention_layernorm.weight | Shape: torch.Size([4096])\n",
      "Layer: model.layers.1.self_attn.q_proj.weight | Shape: torch.Size([4096, 4096])\n",
      "Layer: model.layers.1.self_attn.k_proj.weight | Shape: torch.Size([1024, 4096])\n",
      "Layer: model.layers.1.self_attn.v_proj.weight | Shape: torch.Size([1024, 4096])\n",
      "Layer: model.layers.1.self_attn.o_proj.weight | Shape: torch.Size([4096, 4096])\n",
      "Layer: model.layers.1.mlp.gate_proj.weight | Shape: torch.Size([14336, 4096])\n",
      "Layer: model.layers.1.mlp.up_proj.weight | Shape: torch.Size([14336, 4096])\n",
      "Layer: model.layers.1.mlp.down_proj.weight | Shape: torch.Size([4096, 14336])\n",
      "Layer: model.layers.1.input_layernorm.weight | Shape: torch.Size([4096])\n",
      "Layer: model.layers.1.post_attention_layernorm.weight | Shape: torch.Size([4096])\n",
      "Layer: model.layers.2.self_attn.q_proj.weight | Shape: torch.Size([4096, 4096])\n",
      "Layer: model.layers.2.self_attn.k_proj.weight | Shape: torch.Size([1024, 4096])\n",
      "Layer: model.layers.2.self_attn.v_proj.weight | Shape: torch.Size([1024, 4096])\n",
      "Layer: model.layers.2.self_attn.o_proj.weight | Shape: torch.Size([4096, 4096])\n",
      "Layer: model.layers.2.mlp.gate_proj.weight | Shape: torch.Size([14336, 4096])\n",
      "Layer: model.layers.2.mlp.up_proj.weight | Shape: torch.Size([14336, 4096])\n",
      "Layer: model.layers.2.mlp.down_proj.weight | Shape: torch.Size([4096, 14336])\n",
      "Layer: model.layers.2.input_layernorm.weight | Shape: torch.Size([4096])\n",
      "Layer: model.layers.2.post_attention_layernorm.weight | Shape: torch.Size([4096])\n",
      "Layer: model.layers.3.self_attn.q_proj.weight | Shape: torch.Size([4096, 4096])\n",
      "Layer: model.layers.3.self_attn.k_proj.weight | Shape: torch.Size([1024, 4096])\n",
      "Layer: model.layers.3.self_attn.v_proj.weight | Shape: torch.Size([1024, 4096])\n",
      "Layer: model.layers.3.self_attn.o_proj.weight | Shape: torch.Size([4096, 4096])\n",
      "Layer: model.layers.3.mlp.gate_proj.weight | Shape: torch.Size([14336, 4096])\n",
      "Layer: model.layers.3.mlp.up_proj.weight | Shape: torch.Size([14336, 4096])\n",
      "Layer: model.layers.3.mlp.down_proj.weight | Shape: torch.Size([4096, 14336])\n",
      "Layer: model.layers.3.input_layernorm.weight | Shape: torch.Size([4096])\n",
      "Layer: model.layers.3.post_attention_layernorm.weight | Shape: torch.Size([4096])\n",
      "Layer: model.layers.4.self_attn.q_proj.weight | Shape: torch.Size([4096, 4096])\n",
      "Layer: model.layers.4.self_attn.k_proj.weight | Shape: torch.Size([1024, 4096])\n",
      "Layer: model.layers.4.self_attn.v_proj.weight | Shape: torch.Size([1024, 4096])\n",
      "Layer: model.layers.4.self_attn.o_proj.weight | Shape: torch.Size([4096, 4096])\n",
      "Layer: model.layers.4.mlp.gate_proj.weight | Shape: torch.Size([14336, 4096])\n",
      "Layer: model.layers.4.mlp.up_proj.weight | Shape: torch.Size([14336, 4096])\n",
      "Layer: model.layers.4.mlp.down_proj.weight | Shape: torch.Size([4096, 14336])\n",
      "Layer: model.layers.4.input_layernorm.weight | Shape: torch.Size([4096])\n",
      "Layer: model.layers.4.post_attention_layernorm.weight | Shape: torch.Size([4096])\n",
      "Layer: model.layers.5.self_attn.q_proj.weight | Shape: torch.Size([4096, 4096])\n",
      "Layer: model.layers.5.self_attn.k_proj.weight | Shape: torch.Size([1024, 4096])\n",
      "Layer: model.layers.5.self_attn.v_proj.weight | Shape: torch.Size([1024, 4096])\n",
      "Layer: model.layers.5.self_attn.o_proj.weight | Shape: torch.Size([4096, 4096])\n",
      "Layer: model.layers.5.mlp.gate_proj.weight | Shape: torch.Size([14336, 4096])\n",
      "Layer: model.layers.5.mlp.up_proj.weight | Shape: torch.Size([14336, 4096])\n",
      "Layer: model.layers.5.mlp.down_proj.weight | Shape: torch.Size([4096, 14336])\n",
      "Layer: model.layers.5.input_layernorm.weight | Shape: torch.Size([4096])\n",
      "Layer: model.layers.5.post_attention_layernorm.weight | Shape: torch.Size([4096])\n",
      "Layer: model.layers.6.self_attn.q_proj.weight | Shape: torch.Size([4096, 4096])\n",
      "Layer: model.layers.6.self_attn.k_proj.weight | Shape: torch.Size([1024, 4096])\n",
      "Layer: model.layers.6.self_attn.v_proj.weight | Shape: torch.Size([1024, 4096])\n",
      "Layer: model.layers.6.self_attn.o_proj.weight | Shape: torch.Size([4096, 4096])\n",
      "Layer: model.layers.6.mlp.gate_proj.weight | Shape: torch.Size([14336, 4096])\n",
      "Layer: model.layers.6.mlp.up_proj.weight | Shape: torch.Size([14336, 4096])\n",
      "Layer: model.layers.6.mlp.down_proj.weight | Shape: torch.Size([4096, 14336])\n",
      "Layer: model.layers.6.input_layernorm.weight | Shape: torch.Size([4096])\n",
      "Layer: model.layers.6.post_attention_layernorm.weight | Shape: torch.Size([4096])\n",
      "Layer: model.layers.7.self_attn.q_proj.weight | Shape: torch.Size([4096, 4096])\n",
      "Layer: model.layers.7.self_attn.k_proj.weight | Shape: torch.Size([1024, 4096])\n",
      "Layer: model.layers.7.self_attn.v_proj.weight | Shape: torch.Size([1024, 4096])\n",
      "Layer: model.layers.7.self_attn.o_proj.weight | Shape: torch.Size([4096, 4096])\n",
      "Layer: model.layers.7.mlp.gate_proj.weight | Shape: torch.Size([14336, 4096])\n",
      "Layer: model.layers.7.mlp.up_proj.weight | Shape: torch.Size([14336, 4096])\n",
      "Layer: model.layers.7.mlp.down_proj.weight | Shape: torch.Size([4096, 14336])\n",
      "Layer: model.layers.7.input_layernorm.weight | Shape: torch.Size([4096])\n",
      "Layer: model.layers.7.post_attention_layernorm.weight | Shape: torch.Size([4096])\n",
      "Layer: model.layers.8.self_attn.q_proj.weight | Shape: torch.Size([4096, 4096])\n",
      "Layer: model.layers.8.self_attn.k_proj.weight | Shape: torch.Size([1024, 4096])\n",
      "Layer: model.layers.8.self_attn.v_proj.weight | Shape: torch.Size([1024, 4096])\n",
      "Layer: model.layers.8.self_attn.o_proj.weight | Shape: torch.Size([4096, 4096])\n",
      "Layer: model.layers.8.mlp.gate_proj.weight | Shape: torch.Size([14336, 4096])\n",
      "Layer: model.layers.8.mlp.up_proj.weight | Shape: torch.Size([14336, 4096])\n",
      "Layer: model.layers.8.mlp.down_proj.weight | Shape: torch.Size([4096, 14336])\n",
      "Layer: model.layers.8.input_layernorm.weight | Shape: torch.Size([4096])\n",
      "Layer: model.layers.8.post_attention_layernorm.weight | Shape: torch.Size([4096])\n",
      "Layer: model.layers.9.self_attn.q_proj.weight | Shape: torch.Size([4096, 4096])\n",
      "Layer: model.layers.9.self_attn.k_proj.weight | Shape: torch.Size([1024, 4096])\n",
      "Layer: model.layers.9.self_attn.v_proj.weight | Shape: torch.Size([1024, 4096])\n",
      "Layer: model.layers.9.self_attn.o_proj.weight | Shape: torch.Size([4096, 4096])\n",
      "Layer: model.layers.9.mlp.gate_proj.weight | Shape: torch.Size([14336, 4096])\n",
      "Layer: model.layers.9.mlp.up_proj.weight | Shape: torch.Size([14336, 4096])\n",
      "Layer: model.layers.9.mlp.down_proj.weight | Shape: torch.Size([4096, 14336])\n",
      "Layer: model.layers.9.input_layernorm.weight | Shape: torch.Size([4096])\n",
      "Layer: model.layers.9.post_attention_layernorm.weight | Shape: torch.Size([4096])\n",
      "Layer: model.layers.10.self_attn.q_proj.weight | Shape: torch.Size([4096, 4096])\n",
      "Layer: model.layers.10.self_attn.k_proj.weight | Shape: torch.Size([1024, 4096])\n",
      "Layer: model.layers.10.self_attn.v_proj.weight | Shape: torch.Size([1024, 4096])\n",
      "Layer: model.layers.10.self_attn.o_proj.weight | Shape: torch.Size([4096, 4096])\n",
      "Layer: model.layers.10.mlp.gate_proj.weight | Shape: torch.Size([14336, 4096])\n",
      "Layer: model.layers.10.mlp.up_proj.weight | Shape: torch.Size([14336, 4096])\n",
      "Layer: model.layers.10.mlp.down_proj.weight | Shape: torch.Size([4096, 14336])\n",
      "Layer: model.layers.10.input_layernorm.weight | Shape: torch.Size([4096])\n",
      "Layer: model.layers.10.post_attention_layernorm.weight | Shape: torch.Size([4096])\n",
      "Layer: model.layers.11.self_attn.q_proj.weight | Shape: torch.Size([4096, 4096])\n",
      "Layer: model.layers.11.self_attn.k_proj.weight | Shape: torch.Size([1024, 4096])\n",
      "Layer: model.layers.11.self_attn.v_proj.weight | Shape: torch.Size([1024, 4096])\n",
      "Layer: model.layers.11.self_attn.o_proj.weight | Shape: torch.Size([4096, 4096])\n",
      "Layer: model.layers.11.mlp.gate_proj.weight | Shape: torch.Size([14336, 4096])\n",
      "Layer: model.layers.11.mlp.up_proj.weight | Shape: torch.Size([14336, 4096])\n",
      "Layer: model.layers.11.mlp.down_proj.weight | Shape: torch.Size([4096, 14336])\n",
      "Layer: model.layers.11.input_layernorm.weight | Shape: torch.Size([4096])\n",
      "Layer: model.layers.11.post_attention_layernorm.weight | Shape: torch.Size([4096])\n",
      "Layer: model.layers.12.self_attn.q_proj.weight | Shape: torch.Size([4096, 4096])\n",
      "Layer: model.layers.12.self_attn.k_proj.weight | Shape: torch.Size([1024, 4096])\n",
      "Layer: model.layers.12.self_attn.v_proj.weight | Shape: torch.Size([1024, 4096])\n",
      "Layer: model.layers.12.self_attn.o_proj.weight | Shape: torch.Size([4096, 4096])\n",
      "Layer: model.layers.12.mlp.gate_proj.weight | Shape: torch.Size([14336, 4096])\n",
      "Layer: model.layers.12.mlp.up_proj.weight | Shape: torch.Size([14336, 4096])\n",
      "Layer: model.layers.12.mlp.down_proj.weight | Shape: torch.Size([4096, 14336])\n",
      "Layer: model.layers.12.input_layernorm.weight | Shape: torch.Size([4096])\n",
      "Layer: model.layers.12.post_attention_layernorm.weight | Shape: torch.Size([4096])\n",
      "Layer: model.layers.13.self_attn.q_proj.weight | Shape: torch.Size([4096, 4096])\n",
      "Layer: model.layers.13.self_attn.k_proj.weight | Shape: torch.Size([1024, 4096])\n",
      "Layer: model.layers.13.self_attn.v_proj.weight | Shape: torch.Size([1024, 4096])\n",
      "Layer: model.layers.13.self_attn.o_proj.weight | Shape: torch.Size([4096, 4096])\n",
      "Layer: model.layers.13.mlp.gate_proj.weight | Shape: torch.Size([14336, 4096])\n",
      "Layer: model.layers.13.mlp.up_proj.weight | Shape: torch.Size([14336, 4096])\n",
      "Layer: model.layers.13.mlp.down_proj.weight | Shape: torch.Size([4096, 14336])\n",
      "Layer: model.layers.13.input_layernorm.weight | Shape: torch.Size([4096])\n",
      "Layer: model.layers.13.post_attention_layernorm.weight | Shape: torch.Size([4096])\n",
      "Layer: model.layers.14.self_attn.q_proj.weight | Shape: torch.Size([4096, 4096])\n",
      "Layer: model.layers.14.self_attn.k_proj.weight | Shape: torch.Size([1024, 4096])\n",
      "Layer: model.layers.14.self_attn.v_proj.weight | Shape: torch.Size([1024, 4096])\n",
      "Layer: model.layers.14.self_attn.o_proj.weight | Shape: torch.Size([4096, 4096])\n",
      "Layer: model.layers.14.mlp.gate_proj.weight | Shape: torch.Size([14336, 4096])\n",
      "Layer: model.layers.14.mlp.up_proj.weight | Shape: torch.Size([14336, 4096])\n",
      "Layer: model.layers.14.mlp.down_proj.weight | Shape: torch.Size([4096, 14336])\n",
      "Layer: model.layers.14.input_layernorm.weight | Shape: torch.Size([4096])\n",
      "Layer: model.layers.14.post_attention_layernorm.weight | Shape: torch.Size([4096])\n",
      "Layer: model.layers.15.self_attn.q_proj.weight | Shape: torch.Size([4096, 4096])\n",
      "Layer: model.layers.15.self_attn.k_proj.weight | Shape: torch.Size([1024, 4096])\n",
      "Layer: model.layers.15.self_attn.v_proj.weight | Shape: torch.Size([1024, 4096])\n",
      "Layer: model.layers.15.self_attn.o_proj.weight | Shape: torch.Size([4096, 4096])\n",
      "Layer: model.layers.15.mlp.gate_proj.weight | Shape: torch.Size([14336, 4096])\n",
      "Layer: model.layers.15.mlp.up_proj.weight | Shape: torch.Size([14336, 4096])\n",
      "Layer: model.layers.15.mlp.down_proj.weight | Shape: torch.Size([4096, 14336])\n",
      "Layer: model.layers.15.input_layernorm.weight | Shape: torch.Size([4096])\n",
      "Layer: model.layers.15.post_attention_layernorm.weight | Shape: torch.Size([4096])\n",
      "Layer: model.layers.16.self_attn.q_proj.weight | Shape: torch.Size([4096, 4096])\n",
      "Layer: model.layers.16.self_attn.k_proj.weight | Shape: torch.Size([1024, 4096])\n",
      "Layer: model.layers.16.self_attn.v_proj.weight | Shape: torch.Size([1024, 4096])\n",
      "Layer: model.layers.16.self_attn.o_proj.weight | Shape: torch.Size([4096, 4096])\n",
      "Layer: model.layers.16.mlp.gate_proj.weight | Shape: torch.Size([14336, 4096])\n",
      "Layer: model.layers.16.mlp.up_proj.weight | Shape: torch.Size([14336, 4096])\n",
      "Layer: model.layers.16.mlp.down_proj.weight | Shape: torch.Size([4096, 14336])\n",
      "Layer: model.layers.16.input_layernorm.weight | Shape: torch.Size([4096])\n",
      "Layer: model.layers.16.post_attention_layernorm.weight | Shape: torch.Size([4096])\n",
      "Layer: model.layers.17.self_attn.q_proj.weight | Shape: torch.Size([4096, 4096])\n",
      "Layer: model.layers.17.self_attn.k_proj.weight | Shape: torch.Size([1024, 4096])\n",
      "Layer: model.layers.17.self_attn.v_proj.weight | Shape: torch.Size([1024, 4096])\n",
      "Layer: model.layers.17.self_attn.o_proj.weight | Shape: torch.Size([4096, 4096])\n",
      "Layer: model.layers.17.mlp.gate_proj.weight | Shape: torch.Size([14336, 4096])\n",
      "Layer: model.layers.17.mlp.up_proj.weight | Shape: torch.Size([14336, 4096])\n",
      "Layer: model.layers.17.mlp.down_proj.weight | Shape: torch.Size([4096, 14336])\n",
      "Layer: model.layers.17.input_layernorm.weight | Shape: torch.Size([4096])\n",
      "Layer: model.layers.17.post_attention_layernorm.weight | Shape: torch.Size([4096])\n",
      "Layer: model.layers.18.self_attn.q_proj.weight | Shape: torch.Size([4096, 4096])\n",
      "Layer: model.layers.18.self_attn.k_proj.weight | Shape: torch.Size([1024, 4096])\n",
      "Layer: model.layers.18.self_attn.v_proj.weight | Shape: torch.Size([1024, 4096])\n",
      "Layer: model.layers.18.self_attn.o_proj.weight | Shape: torch.Size([4096, 4096])\n",
      "Layer: model.layers.18.mlp.gate_proj.weight | Shape: torch.Size([14336, 4096])\n",
      "Layer: model.layers.18.mlp.up_proj.weight | Shape: torch.Size([14336, 4096])\n",
      "Layer: model.layers.18.mlp.down_proj.weight | Shape: torch.Size([4096, 14336])\n",
      "Layer: model.layers.18.input_layernorm.weight | Shape: torch.Size([4096])\n",
      "Layer: model.layers.18.post_attention_layernorm.weight | Shape: torch.Size([4096])\n",
      "Layer: model.layers.19.self_attn.q_proj.weight | Shape: torch.Size([4096, 4096])\n",
      "Layer: model.layers.19.self_attn.k_proj.weight | Shape: torch.Size([1024, 4096])\n",
      "Layer: model.layers.19.self_attn.v_proj.weight | Shape: torch.Size([1024, 4096])\n",
      "Layer: model.layers.19.self_attn.o_proj.weight | Shape: torch.Size([4096, 4096])\n",
      "Layer: model.layers.19.mlp.gate_proj.weight | Shape: torch.Size([14336, 4096])\n",
      "Layer: model.layers.19.mlp.up_proj.weight | Shape: torch.Size([14336, 4096])\n",
      "Layer: model.layers.19.mlp.down_proj.weight | Shape: torch.Size([4096, 14336])\n",
      "Layer: model.layers.19.input_layernorm.weight | Shape: torch.Size([4096])\n",
      "Layer: model.layers.19.post_attention_layernorm.weight | Shape: torch.Size([4096])\n",
      "Layer: model.layers.20.self_attn.q_proj.weight | Shape: torch.Size([4096, 4096])\n",
      "Layer: model.layers.20.self_attn.k_proj.weight | Shape: torch.Size([1024, 4096])\n",
      "Layer: model.layers.20.self_attn.v_proj.weight | Shape: torch.Size([1024, 4096])\n",
      "Layer: model.layers.20.self_attn.o_proj.weight | Shape: torch.Size([4096, 4096])\n",
      "Layer: model.layers.20.mlp.gate_proj.weight | Shape: torch.Size([14336, 4096])\n",
      "Layer: model.layers.20.mlp.up_proj.weight | Shape: torch.Size([14336, 4096])\n",
      "Layer: model.layers.20.mlp.down_proj.weight | Shape: torch.Size([4096, 14336])\n",
      "Layer: model.layers.20.input_layernorm.weight | Shape: torch.Size([4096])\n",
      "Layer: model.layers.20.post_attention_layernorm.weight | Shape: torch.Size([4096])\n",
      "Layer: model.layers.21.self_attn.q_proj.weight | Shape: torch.Size([4096, 4096])\n",
      "Layer: model.layers.21.self_attn.k_proj.weight | Shape: torch.Size([1024, 4096])\n",
      "Layer: model.layers.21.self_attn.v_proj.weight | Shape: torch.Size([1024, 4096])\n",
      "Layer: model.layers.21.self_attn.o_proj.weight | Shape: torch.Size([4096, 4096])\n",
      "Layer: model.layers.21.mlp.gate_proj.weight | Shape: torch.Size([14336, 4096])\n",
      "Layer: model.layers.21.mlp.up_proj.weight | Shape: torch.Size([14336, 4096])\n",
      "Layer: model.layers.21.mlp.down_proj.weight | Shape: torch.Size([4096, 14336])\n",
      "Layer: model.layers.21.input_layernorm.weight | Shape: torch.Size([4096])\n",
      "Layer: model.layers.21.post_attention_layernorm.weight | Shape: torch.Size([4096])\n",
      "Layer: model.layers.22.self_attn.q_proj.weight | Shape: torch.Size([4096, 4096])\n",
      "Layer: model.layers.22.self_attn.k_proj.weight | Shape: torch.Size([1024, 4096])\n",
      "Layer: model.layers.22.self_attn.v_proj.weight | Shape: torch.Size([1024, 4096])\n",
      "Layer: model.layers.22.self_attn.o_proj.weight | Shape: torch.Size([4096, 4096])\n",
      "Layer: model.layers.22.mlp.gate_proj.weight | Shape: torch.Size([14336, 4096])\n",
      "Layer: model.layers.22.mlp.up_proj.weight | Shape: torch.Size([14336, 4096])\n",
      "Layer: model.layers.22.mlp.down_proj.weight | Shape: torch.Size([4096, 14336])\n",
      "Layer: model.layers.22.input_layernorm.weight | Shape: torch.Size([4096])\n",
      "Layer: model.layers.22.post_attention_layernorm.weight | Shape: torch.Size([4096])\n",
      "Layer: model.layers.23.self_attn.q_proj.weight | Shape: torch.Size([4096, 4096])\n",
      "Layer: model.layers.23.self_attn.k_proj.weight | Shape: torch.Size([1024, 4096])\n",
      "Layer: model.layers.23.self_attn.v_proj.weight | Shape: torch.Size([1024, 4096])\n",
      "Layer: model.layers.23.self_attn.o_proj.weight | Shape: torch.Size([4096, 4096])\n",
      "Layer: model.layers.23.mlp.gate_proj.weight | Shape: torch.Size([14336, 4096])\n",
      "Layer: model.layers.23.mlp.up_proj.weight | Shape: torch.Size([14336, 4096])\n",
      "Layer: model.layers.23.mlp.down_proj.weight | Shape: torch.Size([4096, 14336])\n",
      "Layer: model.layers.23.input_layernorm.weight | Shape: torch.Size([4096])\n",
      "Layer: model.layers.23.post_attention_layernorm.weight | Shape: torch.Size([4096])\n",
      "Layer: model.layers.24.self_attn.q_proj.weight | Shape: torch.Size([4096, 4096])\n",
      "Layer: model.layers.24.self_attn.k_proj.weight | Shape: torch.Size([1024, 4096])\n",
      "Layer: model.layers.24.self_attn.v_proj.weight | Shape: torch.Size([1024, 4096])\n",
      "Layer: model.layers.24.self_attn.o_proj.weight | Shape: torch.Size([4096, 4096])\n",
      "Layer: model.layers.24.mlp.gate_proj.weight | Shape: torch.Size([14336, 4096])\n",
      "Layer: model.layers.24.mlp.up_proj.weight | Shape: torch.Size([14336, 4096])\n",
      "Layer: model.layers.24.mlp.down_proj.weight | Shape: torch.Size([4096, 14336])\n",
      "Layer: model.layers.24.input_layernorm.weight | Shape: torch.Size([4096])\n",
      "Layer: model.layers.24.post_attention_layernorm.weight | Shape: torch.Size([4096])\n",
      "Layer: model.layers.25.self_attn.q_proj.weight | Shape: torch.Size([4096, 4096])\n",
      "Layer: model.layers.25.self_attn.k_proj.weight | Shape: torch.Size([1024, 4096])\n",
      "Layer: model.layers.25.self_attn.v_proj.weight | Shape: torch.Size([1024, 4096])\n",
      "Layer: model.layers.25.self_attn.o_proj.weight | Shape: torch.Size([4096, 4096])\n",
      "Layer: model.layers.25.mlp.gate_proj.weight | Shape: torch.Size([14336, 4096])\n",
      "Layer: model.layers.25.mlp.up_proj.weight | Shape: torch.Size([14336, 4096])\n",
      "Layer: model.layers.25.mlp.down_proj.weight | Shape: torch.Size([4096, 14336])\n",
      "Layer: model.layers.25.input_layernorm.weight | Shape: torch.Size([4096])\n",
      "Layer: model.layers.25.post_attention_layernorm.weight | Shape: torch.Size([4096])\n",
      "Layer: model.layers.26.self_attn.q_proj.weight | Shape: torch.Size([4096, 4096])\n",
      "Layer: model.layers.26.self_attn.k_proj.weight | Shape: torch.Size([1024, 4096])\n",
      "Layer: model.layers.26.self_attn.v_proj.weight | Shape: torch.Size([1024, 4096])\n",
      "Layer: model.layers.26.self_attn.o_proj.weight | Shape: torch.Size([4096, 4096])\n",
      "Layer: model.layers.26.mlp.gate_proj.weight | Shape: torch.Size([14336, 4096])\n",
      "Layer: model.layers.26.mlp.up_proj.weight | Shape: torch.Size([14336, 4096])\n",
      "Layer: model.layers.26.mlp.down_proj.weight | Shape: torch.Size([4096, 14336])\n",
      "Layer: model.layers.26.input_layernorm.weight | Shape: torch.Size([4096])\n",
      "Layer: model.layers.26.post_attention_layernorm.weight | Shape: torch.Size([4096])\n",
      "Layer: model.layers.27.self_attn.q_proj.weight | Shape: torch.Size([4096, 4096])\n",
      "Layer: model.layers.27.self_attn.k_proj.weight | Shape: torch.Size([1024, 4096])\n",
      "Layer: model.layers.27.self_attn.v_proj.weight | Shape: torch.Size([1024, 4096])\n",
      "Layer: model.layers.27.self_attn.o_proj.weight | Shape: torch.Size([4096, 4096])\n",
      "Layer: model.layers.27.mlp.gate_proj.weight | Shape: torch.Size([14336, 4096])\n",
      "Layer: model.layers.27.mlp.up_proj.weight | Shape: torch.Size([14336, 4096])\n",
      "Layer: model.layers.27.mlp.down_proj.weight | Shape: torch.Size([4096, 14336])\n",
      "Layer: model.layers.27.input_layernorm.weight | Shape: torch.Size([4096])\n",
      "Layer: model.layers.27.post_attention_layernorm.weight | Shape: torch.Size([4096])\n",
      "Layer: model.layers.28.self_attn.q_proj.weight | Shape: torch.Size([4096, 4096])\n",
      "Layer: model.layers.28.self_attn.k_proj.weight | Shape: torch.Size([1024, 4096])\n",
      "Layer: model.layers.28.self_attn.v_proj.weight | Shape: torch.Size([1024, 4096])\n",
      "Layer: model.layers.28.self_attn.o_proj.weight | Shape: torch.Size([4096, 4096])\n",
      "Layer: model.layers.28.mlp.gate_proj.weight | Shape: torch.Size([14336, 4096])\n",
      "Layer: model.layers.28.mlp.up_proj.weight | Shape: torch.Size([14336, 4096])\n",
      "Layer: model.layers.28.mlp.down_proj.weight | Shape: torch.Size([4096, 14336])\n",
      "Layer: model.layers.28.input_layernorm.weight | Shape: torch.Size([4096])\n",
      "Layer: model.layers.28.post_attention_layernorm.weight | Shape: torch.Size([4096])\n",
      "Layer: model.layers.29.self_attn.q_proj.weight | Shape: torch.Size([4096, 4096])\n",
      "Layer: model.layers.29.self_attn.k_proj.weight | Shape: torch.Size([1024, 4096])\n",
      "Layer: model.layers.29.self_attn.v_proj.weight | Shape: torch.Size([1024, 4096])\n",
      "Layer: model.layers.29.self_attn.o_proj.weight | Shape: torch.Size([4096, 4096])\n",
      "Layer: model.layers.29.mlp.gate_proj.weight | Shape: torch.Size([14336, 4096])\n",
      "Layer: model.layers.29.mlp.up_proj.weight | Shape: torch.Size([14336, 4096])\n",
      "Layer: model.layers.29.mlp.down_proj.weight | Shape: torch.Size([4096, 14336])\n",
      "Layer: model.layers.29.input_layernorm.weight | Shape: torch.Size([4096])\n",
      "Layer: model.layers.29.post_attention_layernorm.weight | Shape: torch.Size([4096])\n",
      "Layer: model.layers.30.self_attn.q_proj.weight | Shape: torch.Size([4096, 4096])\n",
      "Layer: model.layers.30.self_attn.k_proj.weight | Shape: torch.Size([1024, 4096])\n",
      "Layer: model.layers.30.self_attn.v_proj.weight | Shape: torch.Size([1024, 4096])\n",
      "Layer: model.layers.30.self_attn.o_proj.weight | Shape: torch.Size([4096, 4096])\n",
      "Layer: model.layers.30.mlp.gate_proj.weight | Shape: torch.Size([14336, 4096])\n",
      "Layer: model.layers.30.mlp.up_proj.weight | Shape: torch.Size([14336, 4096])\n",
      "Layer: model.layers.30.mlp.down_proj.weight | Shape: torch.Size([4096, 14336])\n",
      "Layer: model.layers.30.input_layernorm.weight | Shape: torch.Size([4096])\n",
      "Layer: model.layers.30.post_attention_layernorm.weight | Shape: torch.Size([4096])\n",
      "Layer: model.layers.31.self_attn.q_proj.weight | Shape: torch.Size([4096, 4096])\n",
      "Layer: model.layers.31.self_attn.k_proj.weight | Shape: torch.Size([1024, 4096])\n",
      "Layer: model.layers.31.self_attn.v_proj.weight | Shape: torch.Size([1024, 4096])\n",
      "Layer: model.layers.31.self_attn.o_proj.weight | Shape: torch.Size([4096, 4096])\n",
      "Layer: model.layers.31.mlp.gate_proj.weight | Shape: torch.Size([14336, 4096])\n",
      "Layer: model.layers.31.mlp.up_proj.weight | Shape: torch.Size([14336, 4096])\n",
      "Layer: model.layers.31.mlp.down_proj.weight | Shape: torch.Size([4096, 14336])\n",
      "Layer: model.layers.31.input_layernorm.weight | Shape: torch.Size([4096])\n",
      "Layer: model.layers.31.post_attention_layernorm.weight | Shape: torch.Size([4096])\n",
      "Layer: model.norm.weight | Shape: torch.Size([4096])\n",
      "Layer: lm_head.weight | Shape: torch.Size([128256, 4096])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# also check out this quantized version: neuralmagic/Meta-Llama-3.1-8B-quantized.w8a8\n",
    "# found at: https://huggingface.co/neuralmagic/Meta-Llama-3.1-8B-quantized.w8a8\n",
    "model_name = \"meta-llama/Meta-Llama-3.1-8B\"  # Adjust to a valid model name\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    load_in_4bit=True  # This works if you are using quantized models\n",
    ")\n",
    "\n",
    "# Get the total number of parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "# Print the number of parameters\n",
    "print(f\"Total number of parameters: {total_params:,}\")\n",
    "\n",
    "# Estimate the model size in memory (in GB)\n",
    "# If the model is in float16 (16 bits = 2 bytes), otherwise it's float32 (32 bits = 4 bytes)\n",
    "param_size_in_bytes = 2 if model.dtype == torch.float16 else 4\n",
    "model_size_in_gb = (total_params * param_size_in_bytes) / (1024 ** 3)  # Convert bytes to GB\n",
    "\n",
    "print(f\"Estimated model size: {model_size_in_gb:.2f} GB\")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Shape: {param.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b73301c",
   "metadata": {},
   "source": [
    "### Create the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19da1382",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Ensure pad_token is defined\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # Option 1: Use eos_token as pad_token\n",
    "    # Option 2: Add a new pad_token (uncomment below if you prefer)\n",
    "    # tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    # model.resize_token_embeddings(len(tokenizer))  # Resize the model embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9000b38f",
   "metadata": {},
   "source": [
    "Test the Tokenizer <br>\n",
    "We can remove code below later, it just showed us how to set max lenght"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f0fd339",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "c:\\Users\\USER\\anaconda3\\envs\\myenv\\Lib\\site-packages\\transformers\\generation\\utils.py:1885: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated output: <|begin_of_text|>Sample input text for testing sequence length. More...\n",
      "\n",
      "#include <TestSequenceLength.h>\n",
      "\n",
      "## Detailed Description\n",
      "\n",
      "Sample input text for testing sequence length.\n",
      "\n",
      "This class represents a sample input text for testing sequence length.\n",
      "\n",
      "Definition at line 27 of file TestSequenceLength.h.\n",
      "\n",
      "## Constructor & Destructor Documentation\n",
      "\n",
      "virtual ~ TestSequenceLength (  )\n",
      "\n",
      "virtual\n",
      "\n",
      "## Member Function Documentation\n",
      "\n",
      "virtual void doInitialize (  )\n",
      "\n",
      "virtual\n",
      "\n",
      "Initializes this test.\n",
      "\n",
      "This method initializes the test. The test must be able to run multiple times, so the initialization method must be able to reset the test to a clean state.\n",
      "\n",
      "Reimplemented from Test.\n",
      "\n",
      "Definition at line 34 of file TestSequenceLength.h.\n",
      "\n",
      "virtual void doRunTest (  )\n",
      "\n",
      "virtual\n",
      "\n",
      "Runs this test.\n",
      "\n",
      "This method runs this test. The test must be able to run multiple times, so the runTest method must be able to reset the test to a clean state.\n",
      "\n",
      "Reimplemented from Test.\n",
      "\n",
      "Definition at line 51 of file TestSequenceLength.h.\n",
      "\n",
      "virtual void doShutdown (  )\n",
      "\n",
      "virtual\n",
      "\n",
      "Shuts down this test.\n",
      "\n",
      "This method shuts down this test. The test must be able to run multiple times, so the shutdown method must be able to reset the test to a clean state.\n",
      "\n",
      "Reimplemented from Test.\n",
      "\n",
      "Definition at line 40 of file TestSequenceLength.h.\n",
      "\n",
      "virtual void doTest (  )\n",
      "\n",
      "virtual\n",
      "\n",
      "Tests this test.\n",
      "\n",
      "This method tests this test. The test must be able to run multiple times, so the test method must be able to reset the test to a clean state.\n",
      "\n",
      "Reimplemented from Test.\n",
      "\n",
      "Definition at line 60 of file TestSequenceLength.h.\n",
      "\n",
      "The documentation for this class was generated from the following file:<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "# Example for setting max sequence length later during inference\n",
    "input_text = \"Sample input text for testing sequence length.\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=2048, truncation=True)\n",
    "\n",
    "# Model generation with max length control\n",
    "outputs = model.generate(inputs[\"input_ids\"], max_length=2048)\n",
    "\n",
    "print(\"Generated output:\", tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5035720a",
   "metadata": {},
   "source": [
    "## Replace LoRA and Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4c1f73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=128,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"lm_head\"]\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95fb27ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8f240c",
   "metadata": {},
   "source": [
    "## Dataset Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "312f65f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full dataset size is: 1219201\n",
      "split dataset size is: 1219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1219/1219 [00:00<00:00, 1266.47 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"wikimedia/wikipedia\", \"20231101.ar\", split=\"train\")\n",
    "print(f\"full dataset size is: {len(dataset)}\")\n",
    "\n",
    "# Select 1% of the dataset\n",
    "dataset = dataset.train_test_split(train_size=0.01)[\"train\"]\n",
    "print(f\"split dataset size is: {len(dataset)}\")\n",
    "# Preprocess the data with tokenization and adding labels\n",
    "def formatting_prompts_func(examples):\n",
    "    titles = examples[\"title\"]\n",
    "    texts  = examples[\"text\"]\n",
    "    \n",
    "    # Create prompt using title and text\n",
    "    prompts = [f\"Wikipedia Article\\n### Title: {title}\\n### Article:\\n{text}\\n{tokenizer.eos_token}\" for title, text in zip(titles, texts)]\n",
    "    \n",
    "    # Tokenize and add labels (labels are the same as input_ids for language modeling)\n",
    "    tokenized_inputs = tokenizer(prompts, truncation=True, padding=\"max_length\", max_length=2048)\n",
    "    tokenized_inputs[\"labels\"] = tokenized_inputs[\"input_ids\"].copy()\n",
    "    \n",
    "    return tokenized_inputs\n",
    "\n",
    "# Apply the function to the dataset\n",
    "tokenized_dataset = dataset.map(formatting_prompts_func, batched=True)\n",
    "\n",
    "# Remove unnecessary columns, if any\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"title\", \"url\", \"text\"])\n",
    "\n",
    "# Set the format for PyTorch\n",
    "tokenized_dataset.set_format(\"torch\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6064122d",
   "metadata": {},
   "source": [
    "## Training Using Hugging Face Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b768c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "c:\\Users\\USER\\anaconda3\\envs\\myenv\\Lib\\site-packages\\accelerate\\accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "# Create a data collator to dynamically pad your inputs\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=1, # was 2\n",
    "    gradient_accumulation_steps=4, # was 8\n",
    "    max_steps=2,  # Change this value to train for more steps\n",
    "    learning_rate=5e-5,\n",
    "    fp16=True,  # Mixed precision for faster training on compatible GPUs\n",
    "    logging_steps=1,\n",
    "    weight_decay=0.01,\n",
    "    output_dir=\"./outputs\",\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator  # Add the data collator here\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13dff94d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]c:\\Users\\USER\\anaconda3\\envs\\myenv\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:660: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      " 50%|█████     | 1/2 [04:39<04:39, 279.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 19.2749, 'grad_norm': nan, 'learning_rate': 5e-05, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [08:52<00:00, 263.79s/it]c:\\Users\\USER\\anaconda3\\envs\\myenv\\Lib\\site-packages\\peft\\utils\\save_and_load.py:202: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 19.1539, 'grad_norm': nan, 'learning_rate': 5e-05, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [08:57<00:00, 268.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 537.0836, 'train_samples_per_second': 0.015, 'train_steps_per_second': 0.004, 'train_loss': 19.214414596557617, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2, training_loss=19.214414596557617, metrics={'train_runtime': 537.0836, 'train_samples_per_second': 0.015, 'train_steps_per_second': 0.004, 'total_flos': 750149726896128.0, 'train_loss': 19.214414596557617, 'epoch': 0.006562756357670222})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start the training process\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ec9111",
   "metadata": {},
   "source": [
    "### Evaluate and Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffb59d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model and tokenizer\n",
    "trainer.save_model(output_dir=\"./outputs\")\n",
    "tokenizer.save_pretrained(\"./outputs\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22dc842",
   "metadata": {},
   "source": [
    "## GPU Memory Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb9e958",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
